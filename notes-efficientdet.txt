=========================================================
offical implementation in tensorflow

https://github.com/google/automl/tree/master/efficientdet

pip install -r requirements.txt


consider not using pycocotools since it doesn't work on windows tensorflow/models#5317

https://github.com/cocodataset/cocoapi/issues/169
replace git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI
	with
		git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI

https://github.com/philferriere/cocoapi

pip uninstall tensorflow-gpu
pip install tensorflow
	tensorflow 2.3.1
		cuda 10.1
		cuDNN SDK 7.6
			relaunch cmd to use newly setup env var for cuda

https://www.tensorflow.org/install/source#gpu

no luck yet...
	ton of errors...
K:\ws\automl\efficientdet>python model_inspect.py --model_name=efficientdet-d0 --logdir=logs
Current thread 0x0003ea70 (most recent call first):
  File "C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 2486 in get_attr
  File "C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 3527 in _create_op_helper
  File "C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 3486 in _create_op_internal

=================================================================
pytorch implementation
	https://github.com/rwightman/efficientdet-pytorch

conda create --name efficientdet-pytorch python=3.7
conda activate efficientdet-pytorch
conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
pip install -r requirements.txt

git clone
(efficientdet-pytorch) K:\ws\apex>python setup.py build
(efficientdet-pytorch) K:\ws\apex>python setup.py install


(efficientdet-pytorch) K:\ws\efficientdet-pytorch>python validate.py coco --model tf_efficientdet_d2 --checkpoint tf_efficientdet_d2.pth -b 64
Loaded state_dict from checkpoint 'tf_efficientdet_d2.pth'
Model tf_efficientdet_d2 created, param count: 8097039
Using AMP mixed precision.
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
loading annotations into memory...
Done (t=0.65s)
creating index...
index created!
Test: [   0/78]  Time: 15.524s (15.524s,    4.12/s)
Test: [  10/78]  Time: 2.243s (3.453s,   18.53/s)
Test: [  20/78]  Time: 2.239s (2.879s,   22.23/s)
Test: [  30/78]  Time: 2.248s (2.675s,   23.92/s)
Test: [  40/78]  Time: 2.227s (2.570s,   24.91/s)
Test: [  50/78]  Time: 2.271s (2.509s,   25.51/s)
Test: [  60/78]  Time: 2.333s (2.472s,   25.89/s)
Test: [  70/78]  Time: 2.277s (2.442s,   26.21/s)
Loading and preparing results...
DONE (t=3.92s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=54.70s).
Accumulating evaluation results...
DONE (t=11.81s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.434
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.628
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.463
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.486
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.606
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.538
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.571
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.639
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.747



Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly
because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.
  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")

https://github.com/NVIDIA/apex/issues/835


git clone https://github.com/NVIDIA/apex.git
	python setup.py build --cuda_ext --cpp_ext
	python setup.py install --cuda_ext --cpp_ext
		doesn't work

conda install -c conda-forge nvidia-apex
	still doesn't work
		forget about it


python validate.py coco --model tf_efficientdet_d7x --checkpoint tf_efficientdet_d7x.pth -b 1


https://www.kaggle.com/shonenkov/training-efficientdet
https://www.kaggle.com/shonenkov/inference-efficientdet

conda install -c conda-forge opencv
conda install -c conda-forge albumentations
	but still got
		ModuleNotFoundError: No module named 'albumentations'
			import sys
			sys.executable
				'c:\\python36\\python.exe'
					install jupyter in conda environment


The following packages will be SUPERSEDED by a higher-priority channel:
	?

conda install -c anaconda pandas
conda install -c anaconda scikit-learn

K:\Anaconda3\envs\efficientdet-pytorch\lib\multiprocessing\reduction.py in dump(obj, file, protocol)
     58 def dump(obj, file, protocol=None):
     59     '''Replacement for pickle.dump() using ForkingPickler.'''
---> 60     ForkingPickler(file, protocol).dump(obj)
     61 
     62 #

BrokenPipeError: [Errno 32] Broken pipe

set num_workers to 0


pip install ensemble-boxes


conda install -c anaconda flask
conda install -c conda-forge flask-restful
conda install -c anaconda requests

==================================================================================================================

https://github.com/rwightman/efficientdet-pytorch

=================================================================
Install
=================================================================
download NVIDIA
download conda
git clone http://git.51vr.local/experimental/efficientdet-pytorch
git clone http://git.51vr.local/experimental/albumentations
start up anaconda prompt, execute
	conda create --name efficientdet-pytorch python=3.7
	conda activate efficientdet-pytorch
	conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
	pip install -r requirements.txt
	conda install -c conda-forge nvidia-apex
	conda install -c conda-forge opencv
	conda install -c conda-forge albumentations
	conda install -c conda-forge jupyterlab
	conda install -c anaconda pandas
	conda install -c anaconda scikit-learny
	pip install ensemble-boxes

	conda remove albumentations --force
	cd albumentations
	pip install -e .
	pip uninstall opencv-python-headless

	pip install torch-lr-finder

	conda install -c conda-forge --no-deps shapely
	conda install -c conda-forge jupyterlab

	conda install -c anaconda flask
	conda install -c conda-forge flask-restful
	conda install -c anaconda requests
=====================================================================
Train/Test
=====================================================================
start up anaconda prompt 
	conda activate efficientdet-pytorch
switch to the installed disk
execute 
	cd efficientdet-pytorch
	jupyter notebook
Put training pictures into a folder (E:\efficientdet-pytorch\drone\xxx)，the name of the folder can't use chinese
E:\efficientdet-pytorch\drone>python generate_train_test_set.py 123
open the finetune-drone-train.ipynb adnd execute
Put testing pictures into (E:\efficientdet-pytorch\drone\test)
open the finetune-drone-test.ipynb adnd execute


python track.py _datasets\test\20200901M2_20200903_1152_250m_objs _datasets\test\20200901M2_20200903_1152_250m.MP4
(efficientdet-pytorch) K:\ws\Object-Detection-Metrics>python pascalvoc.py -gt K:\ws\efficientdet-pytorch\_datasets\test\20200901M2_20200907_1202_200m_fixed_dataset -det K:\ws\efficientdet-pytorch\_datasets\test\20200901M2_20200907_1202_200m_fixed_dataset_det -gtformat xyrb -detformat xyrb --savepath _results -t 0.75 -np
(efficientdet-pytorch) D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos>python generate_standard_raw_data.py 20200901M2_20200903_1152_250m_1_objs 20200901M2_20200903_1152_250m_1.mp4 a.csv


python infer.py D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\private_dataset

python pascalvoc.py -gt D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\private_dataset -det D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\private_dataset_det -gtformat xyrb -detformat xyrb --savepath _results -t 0.75 -np


model 005
[IoU 0.50]: mAP: 74.13%
[IoU 0.55]: mAP: 73.34%
[IoU 0.60]: mAP: 72.80%
[IoU 0.65]: mAP: 71.64%
[IoU 0.70]: mAP: 69.42%
[IoU 0.75]: mAP: 64.62%
[IoU 0.80]: mAP: 54.24%
[IoU 0.85]: mAP: 35.36%
[IoU 0.90]: mAP: 11.19%
[IoU 0.95]: mAP: 0.28%
[IoU 0.50:0.95]: mmAP: 0.527

model 007
[IoU 0.50]: mAP: 84.60%
[IoU 0.55]: mAP: 83.89%
[IoU 0.60]: mAP: 83.46%
[IoU 0.65]: mAP: 82.36%
[IoU 0.70]: mAP: 80.82%
[IoU 0.75]: mAP: 77.00%
[IoU 0.80]: mAP: 68.05%
[IoU 0.85]: mAP: 50.90%
[IoU 0.90]: mAP: 22.51%
[IoU 0.95]: mAP: 2.14%
[IoU 0.50:0.95]: mmAP: 0.636

model 013
[IoU 0.50]: mAP: 89.51%
[IoU 0.55]: mAP: 89.13%
[IoU 0.60]: mAP: 88.83%
[IoU 0.65]: mAP: 88.23%
[IoU 0.70]: mAP: 87.23%
[IoU 0.75]: mAP: 84.31%
[IoU 0.80]: mAP: 75.71%
[IoU 0.85]: mAP: 55.98%
[IoU 0.90]: mAP: 24.45%
[IoU 0.95]: mAP: 2.19%
[IoU 0.50:0.95]: mmAP: 0.686

model 018
[IoU 0.50]: mAP: 88.75%
[IoU 0.55]: mAP: 88.56%
[IoU 0.60]: mAP: 88.28%
[IoU 0.65]: mAP: 87.92%
[IoU 0.70]: mAP: 86.84%
[IoU 0.75]: mAP: 83.99%
[IoU 0.80]: mAP: 75.26%
[IoU 0.85]: mAP: 54.52%
[IoU 0.90]: mAP: 24.68%
[IoU 0.95]: mAP: 1.45%
[IoU 0.50:0.95]: mmAP: 0.680

model 021
[IoU 0.50]: mAP: 90.39%
[IoU 0.55]: mAP: 90.17%
[IoU 0.60]: mAP: 90.00%
[IoU 0.65]: mAP: 89.36%
[IoU 0.70]: mAP: 88.11%
[IoU 0.75]: mAP: 85.54%
[IoU 0.80]: mAP: 76.75%
[IoU 0.85]: mAP: 54.48%
[IoU 0.90]: mAP: 23.43%
[IoU 0.95]: mAP: 1.42%
[IoU 0.50:0.95]: mmAP: 0.690

model 023
[IoU 0.50]: mAP: 89.65%
[IoU 0.55]: mAP: 89.59%
[IoU 0.60]: mAP: 89.44%
[IoU 0.65]: mAP: 89.10%
[IoU 0.70]: mAP: 88.40%
[IoU 0.75]: mAP: 86.52%
[IoU 0.80]: mAP: 80.93%
[IoU 0.85]: mAP: 65.53%
[IoU 0.90]: mAP: 36.31%
[IoU 0.95]: mAP: 5.33%
[IoU 0.50:0.95]: mmAP: 0.721



005 [IoU 0.50:0.95]: mmAP: 0.527
007 [IoU 0.50:0.95]: mmAP: 0.636
013 [IoU 0.50:0.95]: mmAP: 0.686
018 [IoU 0.50:0.95]: mmAP: 0.680
021 [IoU 0.50:0.95]: mmAP: 0.690
023 [IoU 0.50:0.95]: mmAP: 0.721



==========================================================================================

detect ped in the changan project
LR: 0.0001
[RESULT]: Train. Epoch: 31, summary_loss: 0.10923, time: 1.6 mins                  
[RESULT]: Val. Epoch: 31, summary_loss: 0.18652, time: 0.3 mins        

LR: 3e-05
[RESULT]: Train. Epoch: 61, summary_loss: 0.10633, time: 1.6 mins                  
[RESULT]: Val. Epoch: 61, summary_loss: 0.18991, time: 0.3 mins            

just use larger lr which converge faster
	and it doesn't harm the final loss.

==========================================================================================

python pascalvoc.py -gt D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\private_dataset -det D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\private_dataset_det -gtformat xyrb -detformat xyrb --savepath _results -t 0.75 -np

model 023

private50 (vehicle length: no normalization)

[IoU 0.50]: mAP: 79.38%
[IoU 0.55]: mAP: 78.74%
[IoU 0.60]: mAP: 78.28%
[IoU 0.65]: mAP: 77.75%
[IoU 0.70]: mAP: 77.10%
[IoU 0.75]: mAP: 75.42%
[IoU 0.80]: mAP: 70.31%
[IoU 0.85]: mAP: 56.75%
[IoU 0.90]: mAP: 31.75%
[IoU 0.95]: mAP: 4.92%
[IoU 0.50:0.95]: mmAP: 0.630


private50 (vehicle length: 50)

[IoU 0.50]: mAP: 82.85%
[IoU 0.55]: mAP: 82.45%
[IoU 0.60]: mAP: 81.97%
[IoU 0.65]: mAP: 81.54%
[IoU 0.70]: mAP: 81.03%
[IoU 0.75]: mAP: 79.49%
[IoU 0.80]: mAP: 73.80%
[IoU 0.85]: mAP: 61.13%
[IoU 0.90]: mAP: 33.87%
[IoU 0.95]: mAP: 4.12%
[IoU 0.50:0.95]: mmAP: 0.662


private50 (vehicle length: 60)

[IoU 0.50]: mAP: 83.92%
[IoU 0.55]: mAP: 83.54%
[IoU 0.60]: mAP: 82.99%
[IoU 0.65]: mAP: 82.64%
[IoU 0.70]: mAP: 82.29%
[IoU 0.75]: mAP: 80.27%
[IoU 0.80]: mAP: 74.75%
[IoU 0.85]: mAP: 60.46%
[IoU 0.90]: mAP: 34.62%
[IoU 0.95]: mAP: 4.67%
[IoU 0.50:0.95]: mmAP: 0.670


private50 (vehicle length: 70)

[IoU 0.50]: mAP: 84.23%
[IoU 0.55]: mAP: 83.63%
[IoU 0.60]: mAP: 83.32%
[IoU 0.65]: mAP: 82.75%
[IoU 0.70]: mAP: 81.86%
[IoU 0.75]: mAP: 79.24%
[IoU 0.80]: mAP: 71.97%
[IoU 0.85]: mAP: 57.16%
[IoU 0.90]: mAP: 32.52%
[IoU 0.95]: mAP: 3.86%
[IoU 0.50:0.95]: mmAP: 0.661


private50 (vehicle length: 80)

[IoU 0.50]: mAP: 84.17%
[IoU 0.55]: mAP: 83.51%
[IoU 0.60]: mAP: 82.89%
[IoU 0.65]: mAP: 82.03%
[IoU 0.70]: mAP: 80.45%
[IoU 0.75]: mAP: 76.95%
[IoU 0.80]: mAP: 68.78%
[IoU 0.85]: mAP: 51.73%
[IoU 0.90]: mAP: 24.27%
[IoU 0.95]: mAP: 2.12%
[IoU 0.50:0.95]: mmAP: 0.637


private50 (vehicle length: 90)

[IoU 0.50]: mAP: 82.79%
[IoU 0.55]: mAP: 82.04%
[IoU 0.60]: mAP: 81.19%
[IoU 0.65]: mAP: 79.78%
[IoU 0.70]: mAP: 78.08%
[IoU 0.75]: mAP: 73.84%
[IoU 0.80]: mAP: 64.30%
[IoU 0.85]: mAP: 44.26%
[IoU 0.90]: mAP: 17.91%
[IoU 0.95]: mAP: 1.10%
[IoU 0.50:0.95]: mmAP: 0.605

------------------------------------------
model 021
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 83.70%
[IoU 0.55]: mAP: 83.24%
[IoU 0.60]: mAP: 82.77%
[IoU 0.65]: mAP: 82.33%
[IoU 0.70]: mAP: 81.50%
[IoU 0.75]: mAP: 78.29%
[IoU 0.80]: mAP: 69.97%
[IoU 0.85]: mAP: 51.85%
[IoU 0.90]: mAP: 25.01%
[IoU 0.95]: mAP: 1.98%
[IoU 0.50:0.95]: mmAP: 0.641
------------------------------------------
model 005
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 75.76%
[IoU 0.55]: mAP: 75.00%
[IoU 0.60]: mAP: 74.27%
[IoU 0.65]: mAP: 73.40%
[IoU 0.70]: mAP: 72.07%
[IoU 0.75]: mAP: 69.47%
[IoU 0.80]: mAP: 61.89%
[IoU 0.85]: mAP: 44.38%
[IoU 0.90]: mAP: 18.65%
[IoU 0.95]: mAP: 0.92%
[IoU 0.50:0.95]: mmAP: 0.566

model 007
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 79.37%
[IoU 0.55]: mAP: 78.64%
[IoU 0.60]: mAP: 78.06%
[IoU 0.65]: mAP: 77.47%
[IoU 0.70]: mAP: 76.31%
[IoU 0.75]: mAP: 74.35%
[IoU 0.80]: mAP: 68.88%
[IoU 0.85]: mAP: 56.17%
[IoU 0.90]: mAP: 29.02%
[IoU 0.95]: mAP: 2.31%
[IoU 0.50:0.95]: mmAP: 0.621

model 013
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 83.03%
[IoU 0.55]: mAP: 82.73%
[IoU 0.60]: mAP: 82.35%
[IoU 0.65]: mAP: 81.78%
[IoU 0.70]: mAP: 81.16%
[IoU 0.75]: mAP: 78.44%
[IoU 0.80]: mAP: 69.72%
[IoU 0.85]: mAP: 51.34%
[IoU 0.90]: mAP: 25.69%
[IoU 0.95]: mAP: 2.49%
[IoU 0.50:0.95]: mmAP: 0.639

model 018
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 83.01%
[IoU 0.55]: mAP: 82.62%
[IoU 0.60]: mAP: 82.19%
[IoU 0.65]: mAP: 81.54%
[IoU 0.70]: mAP: 80.83%
[IoU 0.75]: mAP: 77.94%
[IoU 0.80]: mAP: 69.45%
[IoU 0.85]: mAP: 51.30%
[IoU 0.90]: mAP: 24.11%
[IoU 0.95]: mAP: 1.40%
[IoU 0.50:0.95]: mmAP: 0.634

model 021
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 83.70%
[IoU 0.55]: mAP: 83.24%
[IoU 0.60]: mAP: 82.77%
[IoU 0.65]: mAP: 82.33%
[IoU 0.70]: mAP: 81.50%
[IoU 0.75]: mAP: 78.29%
[IoU 0.80]: mAP: 69.97%
[IoU 0.85]: mAP: 51.85%
[IoU 0.90]: mAP: 25.01%
[IoU 0.95]: mAP: 1.98%
[IoU 0.50:0.95]: mmAP: 0.641

model 023
private50 (vehicle length: 60)
[IoU 0.50]: mAP: 83.92%
[IoU 0.55]: mAP: 83.54%
[IoU 0.60]: mAP: 82.99%
[IoU 0.65]: mAP: 82.64%
[IoU 0.70]: mAP: 82.29%
[IoU 0.75]: mAP: 80.27%
[IoU 0.80]: mAP: 74.75%
[IoU 0.85]: mAP: 60.46%
[IoU 0.90]: mAP: 34.62%
[IoU 0.95]: mAP: 4.67%
[IoU 0.50:0.95]: mmAP: 0.670


private50 with incorrect weighted box fusion 0.01 0.009
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+
|  model |   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95 |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+
| 005 60 | 75.76% | 75.00% | 74.27% | 73.40% | 72.07% | 69.47% | 61.89% | 44.38% | 18.65% |  0.92% | 56.6% |
| 007 60 | 79.37% | 78.64% | 78.06% | 77.47% | 76.31% | 74.35% | 68.88% | 56.17% | 29.02% |  2.31% | 62.1% |
| 013 60 | 83.03% | 82.73% | 82.35% | 81.78% | 81.16% | 78.44% | 69.72% | 51.34% | 25.69% |  2.49% | 63.9% |
| 018 60 | 83.01% | 82.62% | 82.19% | 81.54% | 80.83% | 77.94% | 69.45% | 51.30% | 24.11% |  1.40% | 63.4% |
| 021 60 | 83.70% | 83.24% | 82.77% | 82.33% | 81.50% | 78.29% | 69.97% | 51.85% | 25.01% |  1.98% | 64.1% |
| 023 60 | 83.92% | 83.54% | 82.99% | 82.64% | 82.29% | 80.27% | 74.75% | 60.46% | 34.62% |  4.67% | 67.0% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+

private50 without weighted box fusion
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|  model |   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 005 60 | 85.98% | 85.56% | 85.18% | 84.46% | 83.23% | 80.44% | 71.86% | 52.20% | 21.82% |  1.00% | 65.17% |
| 007 60 | 92.04% | 91.81% | 91.36% | 90.75% | 89.66% | 87.54% | 81.24% | 66.21% | 34.68% |  2.86% | 72.81% |
| 013 60 | 95.36% | 95.27% | 94.94% | 94.38% | 93.69% | 90.75% | 81.13% | 60.42% | 30.18% |  2.71% | 73.88% |
| 018 60 | 94.62% | 94.44% | 94.30% | 93.75% | 92.97% | 89.75% | 80.45% | 59.86% | 28.15% |  1.69% | 73.00% |
| 021 60 | 95.35% | 95.13% | 94.78% | 94.37% | 93.39% | 90.00% | 80.38% | 60.58% | 29.52% |  2.35% | 73.58% |
| 023 60 | 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.76% | 71.14% | 40.31% |  5.05% | 77.58% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+


005_768_1536_bs4_epoch6
006_768_1536_rotated_obb_no_cutout_bs2
007_768_1536_rotated_obb_no_cutout_more_bus_bs4
010_768_1536_rotated_obb_no_cutout_more_bus_tongji_bs4
012_768_1536_rotated_obb_no_cutout_more_bus_tong_more_color_gray_blur_aug_lr1e-4_bs4
013_768_1536_rotated_obb_no_cutout_more_bus_tong_changtai_jinqiao_colorjitter0.2_lr1e-4_bs4
018_ysq1_768_less_colorjitter_lr1e-4
021_ysq1_768_1440_less_colorjitter_lr5e-5
023_gm7_syq4_768_lr1e-4

private50
model 023 vehicle size 60
without weighted box fusion
| 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.76% | 71.14% | 40.31% |  5.05% | 77.58% |
with weighted box fusion
iou_thr=0.5 skip_box_thr=0.009
| 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.76% | 71.14% | 40.31% |  5.05% | 77.58% |
iou_thr=0.44 skip_box_thr=0.009
| 95.85% | 95.82% | 95.61% | 95.21% | 94.81% | 92.67% | 86.48% | 70.91% | 40.22% |  5.03% | 77.26% |
iou_thr=0.44 skip_box_thr=0.0
| 96.33% | 96.26% | 95.99% | 95.56% | 95.14% | 92.86% | 86.50% | 70.73% | 40.03% |  5.03% | 77.44% |
iou_thr=0.55 skip_box_thr=0.0
| 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.76% | 71.14% | 40.31% |  5.05% | 77.58% |
iou_thr=0.55 skip_box_thr=0.1
| 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.76% | 71.14% | 40.31% |  5.05% | 77.58% |

model 005 vehicle size 60
without weighted box fusion
| 85.98% | 85.56% | 85.18% | 84.46% | 83.23% | 80.44% | 71.86% | 52.20% | 21.82% |  1.00% | 65.17% |
iou_thr=0.55 skip_box_thr=0.1
| 85.98% | 85.56% | 85.18% | 84.46% | 83.23% | 80.44% | 71.86% | 52.20% | 21.82% |  1.00% | 65.17% |
iou_thr=0.44 skip_box_thr=0.1
| 85.79% | 85.39% | 84.82% | 83.99% | 82.67% | 79.75% | 70.89% | 51.27% | 21.50% |  0.99% | 64.71% |
iou_thr=0.44 skip_box_thr=0.43
| 84.52% | 84.18% | 83.78% | 83.20% | 82.09% | 79.52% | 71.09% | 51.67% | 21.69% |  1.00% | 64.27% |

private50
model 023 no vehicle size normalization
| 90.17% | 90.07% | 89.80% | 89.26% | 88.40% | 86.24% | 80.53% | 64.80% | 35.60% |  5.19% | 72.01% |
	much more false positive and uncertain predictions
model 023, no weighted box fusion, vehicle size normalizated to 50
| 96.08% | 95.94% | 95.69% | 95.35% | 94.80% | 92.92% | 86.84% | 72.37% | 40.06% |  4.53% | 77.46% |
model 023, no weighted box fusion, vehicle size normalizated to 60
| 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.76% | 71.14% | 40.31% |  5.05% | 77.58% |
model 023, no weighted box fusion, vehicle size normalizated to 70
| 96.04% | 95.94% | 95.60% | 94.99% | 93.89% | 91.07% | 83.15% | 66.36% | 37.57% |  4.17% | 75.88% |

so the hyper-parameter best choice is still
	without weighted box fusion, vehicle size normalizated to 60 pixels

====================================================================================================
DONE

45度两车并排
45度大小车并排
相互交重的识别框相互干扰
	weighted box fusion
		iou threshold 0.01 不正确
			0.55 或不用weighted box fusion private50上都能得到最好mmAP
====================================================================================================

现有效果改进
	提高识别速度
	个别超长车辆可能因crop被切断
	大车识别框有时不够准确
	有较多误识别
	稀有车辆

添加功能
	车辆种类、颜色、车型、属性
	OBB
	倾斜拍摄
		低角度视角结果还很差

测试gm7查看大车准确度
	soso
阴影下车辆准确度
模型集成?
整理数据集
整理航拍视频
====================================================================================================

pickup
frontview
multi-section bus/truck
dump truck
empty truck
side view
black car with hard shadow

python pascalvoc.py -gt D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\web-collection-001-002_dataset -det D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\web-collection-001-002_det -gtformat xyrb -detformat xyrb --savepath _results -t 0.75 -np 60

web-collection-001-002
model 023 no vehicle size normalization
| 78.82% | 77.89% | 76.91% | 75.25% | 72.49% | 67.56% | 56.65% | 38.80% | 16.79% |  1.57% | 56.27% |
model 023, no weighted box fusion, vehicle size normalizated to 60 (web-collection-001-002_old 64 pics)
| 91.06% | 90.49% | 89.79% | 88.62% | 86.29% | 81.31% | 71.25% | 50.14% | 23.17% |  1.97% | 67.41% |

model 023, no weighted box fusion, vehicle size normalizated to 60 (web-collection-001-002_ 92 pics)
| 91.72% | 91.44% | 90.77% | 89.69% | 87.76% | 83.27% | 73.10% | 52.12% | 23.07% |  1.51% | 68.44% |


数量
清晰程度

批量下载
批量识别
识别效果差优先标注
数据集构建，分阶段
训练中评价自动（多数据集）

python -m efficientdet-pytorch.infer efficientdet-pytorch/_datasets/_validation/private_dataset_no_crop_aabb 60
python Object-Detection-Metrics/pascalvoc.py -gt ../efficientdet-pytorch/_datasets/_validation/private_dataset_no_crop_aabb -det ../efficientdet-pytorch/_datasets/_validation/private_dataset_no_crop_aabb_det -gtformat xyrb -detformat xyrb --savepath _results -t 0.75 -np

python -u eval_mAP.py efficientdet-pytorch/_datasets/_validation/private_dataset_no_crop_aabb efficientdet-pytorch/_models/effdet-d2-drone_


clear ; python -u eval_mAP.py efficientdet-pytorch/_datasets/_validation/private_dataset_no_crop_aabb efficientdet-pytorch/_models/027_effdet-d2-drone_batch8_lr1e-4/
clear ; python -u eval_mAP.py efficientdet-pytorch/_datasets/_validation/web-collection-001-002_dataset_no_crop_aabb efficientdet-pytorch/_models/027_effdet-d2-drone_batch8_lr1e-4

clear ; python -u eval_mAP.py efficientdet-pytorch/_datasets/_validation/private_dataset_no_crop_aabb efficientdet-pytorch/_models/023/ 2>/dev/null | tee -a 023.log
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 96.36% | 96.26% | 95.99% | 95.63% | 95.15% | 93.00% | 86.80% | 71.24% | 40.18% |  5.15% | 77.58% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
clear ; python -u eval_mAP.py efficientdet-pytorch/_datasets/_validation/web-collection-001-002_dataset_no_crop_aabb efficientdet-pytorch/_models/023/ 2>/dev/null | tee -a 023.log
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 92.37% | 92.04% | 91.43% | 90.32% | 88.48% | 83.90% | 73.66% | 53.07% | 23.19% |  1.56% | 69.00% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+

(efficientdet-pytorch) K:\ws>python -m efficientdet_pytorch.infer D:\git\CyTrafficEditor\Cybertron\Modules\Foundation\Source\CyTrafficEditor\CyTraffic\data\videos\private 60

====================================================================================================
ablation study

77_mAP_uint8_aug
033 full                                   [77.08%],[67.85%]   76.11% , 67.32%     75.58% , 66.02%    75.38% , 66.94%    73.05% , 64.57%    73.13%,64.75%
033 full                                   [77.60%],[68.33%]   75.20% , 65.47%    [78.04%], 67.69%    76.59% , 67.14%    75.46% , 66.21%    72.92%,62.60%
033 full                                   [77.08%],[67.85%]   76.11% , 67.32%     75.58% , 66.02%    75.38% , 66.94%    73.05% , 64.57%    73.13%,64.75%
029 full_float32_aug                       [77.47%],[68.36%]   74.91% , 64.52%     77.00% , 66.57%    77.13% , 68.27%    76.60% , 66.05%    74.24%,63.29%    

034 nocolorjitter                           75.64% , 64.70%    75.98% , 66.17%    [76.25%],65.36%    74.84%,[66.74%]
034 nocolorjitter                          [75.53%], 65.47%    74.30% , 65.28%     73.37% ,63.86%    73.75%, 66.15%      74.18% ,[65.77%]   73.42%,62.19%
034 nocolorjitter                          [75.82%], 65.79%    73.85% , 65.67%     75.32% ,66.15%    75.04%,[67.37%]     73.16% , 65.11%    74.52%,66.10%

035 nocolorjitter_noblur                    75.36% , 66.97%    76.77% , 66.53%   [76.99%],[68.10%]   76.30% , 67.19%
035 nocolorjitter_noblur(2)                 76.42% , 67.31%   [77.94%],[68.33%]   76.68% , 67.85%    76.74% , 67.27%
035 nocolorjitter_noblur                    76.41% , 66.89%    76.70% , 65.99%    76.67% , 66.73%   [76.95%],[67.88%]
035 nocolorjitter_noblur                    73.89% , 65.09%    75.00% , 66.91%    74.97% , 66.71%   [75.07%],[67.41%]

036 nocolorjitter_noblur_noflip             75.49% , 64.62%   [76.32%],[66.45%]   76.24% , 65.26%   74.98% , 65.07%
036 nocolorjitter_noblur_noflip             75.12% , 65.63%   [76.16%],[66.60%]   75.78% , 65.10%   73.44% , 62.96%
036 nocolorjitter_noblur_noflip            [76.95%],[66.98%]   75.01% , 64.49%    75.99% , 66.14%   76.49% , 65.54%

037 nocolorjitter_noblur_noflip_norotation  71.74% , 58.41%   [74.89%],[61.35%]   74.30% , 61.07%   74.23% , 60.72%
037 nocolorjitter_noblur_noflip_norotation  71.89% , 60.39%    74.23% ,[62.13%]   73.69% , 60.43%  [74.93%], 61.99%
037 nocolorjitter_noblur_noflip_norotation  72.75% , 61.18%   [75.14%],[63.52%]   74.39% , 62.26%   74.75% , 62.12%

038_noblur                                  75.33% , 66.05%    75.31% ,[67.25%]   75.81% , 66.34%   75.34% , 66.10%   75.38% , 66.50%  [76.06%], 66.44%

039_noblur_train0.95                        75.48% , 66.77%    75.15% , 67.77%   [77.27%], 67.13%   76.46% ,[68.47%]  73.79% , 64.60%   75.26% , 66.19%

040_noblur_train0.95_batchsize8             75.74% , 67.57%    76.29% ,[68.37%]  [76.95%], 68.18%   74.57% , 65.96%   75.02% , 66.62%   75.54% , 66.10%
041_noblur_train0.95_batchsize8_lr2e-4      75.73% , 66.55%   [76.54%], 65.29%   [76.54%],[68.40%]  75.26% , 65.74%   75.13% , 66.58%   75.76% , 64.66%
042_noblur_train0.95_batchsize8_lr5e-5      75.79% , 68.04%    75.73% , 67.52%   [76.17%],[68.99%]  75.28% , 67.28%   74.73% , 67.84%   75.67% , 68.37%

043_train0.95                               74.98% , 65.93%    74.54% , 66.63%    74.80% , 65.99%  [75.88%],[67.55%]  73.29% , 62.69%   74.72% , 65.67%
044_train0.95_rotate0.8                    [75.69%],[67.13%]   74.11% , 67.01%    75.27% , 66.40%   75.13% , 66.74%   73.35% , 66.61%   73.41% , 64.49%
045_train0.95_isonoise                      75.19% , 66.53%    75.80% ,[67.88%]   75.99% , 65.04%  [76.41%], 66.32%   74.57% , 65.36%   75.40% , 65.52%
                       round2               74.16% , 65.96%    73.99% , 64.82%   [76.93%],[66.98%]  75.34% , 67.41%   74.80% , 66.26%   74.22% , 65.02%
046_train0.95_jpegcompress                  73.77% , 65.49%    72.92% , 65.66%    74.48% , 64.68%   75.92% , 66.02%
                              round2        75.77% , 66.92%   [76.66%], xx.xx%    75.16% , 66.89%   76.01% , 68.41%   74.85% , 67.93%   74.53% , 65.12%
047_train0.95_gaussnoise
                       round2               xx.xx% , 66.57%    74.86% , 65.63%   [75.84%], 66.11%   75.72% , 67.50%  [75.83%],[68.10%]  74.85% , 65.90%
048_train0.95_randomgamma
                       round2               74.88% , 66.13%    xx.xx% , 66.57%    75.40% , 66.84%   76.46% , 68.83%   74.52% , 67.17%   72.89% , 64.71%
043_train0.95_bs8                           76.21% , 67.85%    74.83% , 65.19%   [77.38%],[68.85%]  74.68% , 65.39%   75.37% , 67.45%   75.70% , 65.96%
                                            76.22% , 68.82%    74.74% , 66.23%   [76.50%],[67.74%]  74.33% , 65.68%   75.16% , 67.26%   75.52% , 65.42%
044_train0.95_rotate0.8_bs8                [76.48%],[68.68%]   76.23% , 68.03%    76.21% , 68.47%   74.48% , 66.11%   73.37% , 65.66%   75.03% , 65.59%
                                           [76.88%],[69.01%]   76.00% , 67.88%    76.21% , 68.35%   73.23% , 64.70%   73.24% , 64.48%   74.11% , 65.16%
045_train0.95_isonoise_bs8
046_train0.95_jpegcompress_bs8             [77.40%],[68.17%]   73.66% , 64.43%    75.08% , 67.41%   73.44% , 63.53%   73.37% , 64.01%   74.19% , 65.22%
                                           [77.30%],[68.43%]   74.73% , 64.73%    75.64% , 67.21%   73.26% , 63.97%   73.75% , 64.78%   73.76% , 66.30%
047_train0.95_gaussnoise_bs8               [76.84%],[68.09%]   76.54% , 68.62%    75.92% , 68.66%   75.56% , 65.74%   74.30% , 66.45%   75.24% , 65.04%
                                           [77.22%],[69.27%]   76.21% , 68.08%    75.95% , 69.25%   76.11% , 66.76%   74.84% , 67.17%   74.62% , 66.24%
048_train0.95_randomgamma_bs8              [76.32%],[67.81%]   74.69% , 65.45%    75.51% , 67.30%   74.26% , 65.43%   73.26% , 65.42%   75.86% , 67.77%
                                           [76.92%],[68.83%]   74.45% , 65.72%    74.72% , 68.24%   73.39% , 62.91%   72.88% , 65.28%

049_train0.8_shiftscale0.5                  75.00% , 65.89%   [75.88%],[67.50%]   75.82% , 65.48%   75.78% , 65.36%   75.07% , 66.52%   74.49% , 64.93%
                   round2                   75.00% , 65.89%   [75.88%],[67.50%]   75.82% , 65.48%   75.78% , 65.36%   75.07% , 66.52%   74.49% , 64.93%
                   round3                   75.00% , 65.89%   [75.88%],[67.50%]   75.82% , 65.48%   75.78% , 65.36%   75.07% , 66.52%   74.49% , 64.93%
                   round5    (3090)        [76.85%],[67.91%]   75.75% , 66.50%    74.74% , 64.30%   75.70% , 66.45%   76.11% , 67.34%   75.83% , 65.57%
                   round6    (3090)         77.43% ,[67.97%]  [77.51%], 66.67%    74.01% , 64.06%   76.77% , 67.17%   76.29% , 66.58%   76.03% , 64.83%

050_train0.8_rotate0.8                      74.41% , 65.45%    73.76% , 64.98%   [75.65%],[68.34%]  75.02% , 66.55%   75.20% , 67.72%   75.10% , 65.29%
                                            75.30% ,[68.37%]   75.09% , 67.51%   [76.00%], 67.94%   74.57% , 65.40%   75.88% , 67.74%   73.65% , 66.25%
          055_     round2                   75.39% , 66.44%    74.62% , 65.76%    74.89% , 65.95%  [76.28%], 67.00%   74.90% ,[67.16%]  75.38% , 65.68%
          055_     round2                   75.52% , 68.75%     74.48% , 67.24%   74.49% , 66.59%   75.87% , 67.47%   75.49% , 67.55%  75.00% , 65.82%
          050_     round3                   75.39% , 66.44%    74.62% , 65.76%    74.89% , 65.95%  [76.28%], 67.00%   74.90% ,[67.16%]  75.38% , 65.68%

060_train0.8_perspective0.5 (2080ti)        74.35% , 64.62%    76.43% , 67.25%    77.23% , 67.92%  [77.37%],[68.65%]  77.06% , 67.68%   77.02% , 67.38%
                   round2   (2080ti)        74.35% , 64.62%    76.43% , 67.25%    77.23% , 67.92%  [77.37%],[68.65%]  77.06% , 67.68%   77.02% , 67.38%
                   round3   (2080ti)        74.35% , 64.62%    76.43% , 67.25%    77.23% , 67.92%  [77.37%],[68.65%]  77.06% , 67.68%   77.02% , 67.38%
                   round4    (3090)        [77.33%],[68.16%]   74.74% , 65.44%    76.16% , 65.79%   76.04% , 66.98%   75.33% , 65.72%   73.82% , 64.28%
                   round5    (3090) lost    76.65% , 67.30%    74.44% , 65.10%   [77.04%],[66.80%]   75.20% , 65.95%    75.72% , 66.23%    72.65% , 62.92%   
                   round5    (3090)        [76.68%],[67.28%]   74.44% , 64.33%    76.14% , 67.20%    75.01% , 67.14%    74.38% , 65.57%    72.36% , 63.57%   

059_train0.8_randomgamma     (3090) lost    xx.xx% , 66.68%    75.97% , 67.75%   [76.49%], 67.07%   76.24% , 68.39%   75.81% ,[68.51%]  73.19% , 64.36% 
                                    lost    74.84% , 65.86%    76.25% , xx.xx%    75.92% , 64.99%  [76.56%],[67.79%]  75.31% , 66.97%   74.00% , 64.65%
                                            74.88% , 66.13%   [76.47%], 66.57%    75.40% , 66.84%  [76.46%],[68.83%]  74.52% , 67.17%   72.89% , 64.71%


056_train0.8_isonoise
057_train0.8_jpegcompress
058_train0.8_gaussnoise



051_train0.4                                76.06% , 67.71%    76.80% ,[69.03%]  [77.90%],[69.01%]  75.92% , 65.76%   76.83% , 66.89%   75.56% , 66.56%
                   round2                   76.06% , 67.71%    76.80% ,[69.03%]  [77.90%],[69.01%]  75.92% , 65.76%   76.83% , 66.89%   75.56% , 66.56%
                   round3                   75.50% , 67.94%    76.78% , 68.15%   [77.44%],[68.80%]  73.84% , 64.04%   76.82% , 66.73%   75.35% , 65.93%

052_train0.5                                74.19% , 63.92%   [77.87%],[69.26%]   76.56% , 67.12%   76.88% , 67.89%   75.58% , 66.21%   75.28% , 65.44%
                   round2                   74.49% , 62.88%   [78.05%],[69.32%]   75.61% , 65.93%   75.86% , 67.39%   74.87% , 66.97%   74.80% , 66.24%
                   round3                   74.19% , 63.92%   [77.87%],[69.26%]   76.56% , 67.12%   76.88% , 67.89%   75.58% , 66.21%   75.28% , 65.44%

053_train0.6                                74.84% , 66.32%   [78.61%],[69.70%]   75.98% , 66.82%   76.42% , 67.73%   74.52% , 65.89%   75.52% , 66.68%
                   round2                   74.19% , 63.92%   [77.87%],[69.26%]   76.56% , 67.12%   76.88% , 67.89%   75.58% , 66.21%   75.28% , 65.44%
                   round3                   74.82% , 64.66%   [77.85%],[69.75%]   76.28% , 67.43%   75.88% , 67.72%   74.79% , 65.56%   74.83% , 65.94%

054_train0.7                                76.56% ,[68.52%]   75.32% , 65.80%   [77.36%],[68.75%]  74.01% , 64.30%  [77.44%], 67.54%   76.33% , 66.42%
                   round2                   76.51% , 67.88%    76.03% , 66.82%   [77.45%],[68.38%]  74.52% , 64.69%   77.29% , 67.36%   76.06% , 67.51%
                   round3                   76.56% , 68.52%    75.32% , 65.80%   [77.36%],[68.75%]  74.01% , 64.30%  [77.44%], 67.54%   76.33% , 66.42%


---------------------------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
061_train0.95_virtual_epoch_10k_bs4_w4 |74.20,64.11 73.79,66.27 77.28,68.38 76.24,67.69 72.99,64.47 75.19,67.32 72.19,64.94 77.94,67.91 77.11,67.22 75.49,66.41 73.62,64.82
                                       |                        *                              |                            #           *               |                                                       |                                                        |
061_train0.95_virtual_epoch_10k_bs4_w4 |73.83,64.36 73.48,65.00 76.92,67.70 76.87,67.52 72.26,63.05 73.94,65.02 72.24,62.57 78.23,67.87 76.91,67.31 74.87,66.68 74.46,63.89 75.11,66.24 76.48,66.85 77.63,68.69 74.66,65.93 75.02,65.79 76.66,67.17 74.23,66.20 75.30,65.87 75.65,66.58
                                       |                        *                              |                            #                           |                                           *           |                       *                                |
062_train0.95_virtual_epoch_10k_bs4_w1 |71.86,62.70 74.14,65.98 77.61,68.71 74.89,66.25 75.49,66.88 74.40,67.24 75.27,67.51 77.74,69.00 76.94,68.85 76.52,68.18 73.88,64.26 76.91,67.98 75.64,66.39 76.61,67.88 75.65,66.08 74.93,66.33 75.17,66.25 73.04,66.28 76.90,68.41 75.54,64.93
                                       |                        *                              |                            #                           |                   *                       *           |*                      *                       *        |
---------------------------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
063_train0.95_virtual_epoch_10k_bs8_w8 |72.68,63.54 73.60,65.42 73.54,63.99 76.82,67.30 75.08,65.66 75.94,67.52 74.58,65.96 74.95,66.92 77.67,67.98 75.81,66.80 75.47,65.95 75.39,67.27 75.78,67.71 76.74,67.83 75.18,64.46 76.21,66.86 75.23,66.68 75.45,66.52 75.42,66.31 73.56,64.07
                                       |                                    *                  |                                        #               |                                           *           |           *                                            |
063_train0.95_virtual_epoch_10k_bs8_w8 |72.44,64.94 74.66,66.68 74.50,66.22 76.81,69.26 76.13,67.82 76.69,68.85 74.68,66.46 75.97,68.14 78.21,70.30 76.22,67.83 76.27,66.88 75.13,67.12 76.75,68.87 77.35,69.33 74.87,65.58 75.23,67.59 75.22,67.87 75.62,68.32 74.88,66.29 74.24,64.78
                                       |                                    *           *      |                                        #               |                                           *           |           *           *                                |
064_train0.95_virtual_epoch_10k_bs8_w1 |70.47,64.10 74.01,66.03 74.45,66.41 75.45,68.16 74.28,66.40 77.45,69.75 74.70,66.32 76.90,68.95 76.33,67.08 76.81,68.24 74.83,66.24 75.49,66.57 75.30,66.30 76.43,67.69 76.69,68.81 76.32,68.35 75.29,67.47 74.09,66.89 76.32,67.70 74.92,64.26
                                       |                                    *                  |    #                       *                       *   |                                           *           |*          *                                   *        |
---------------------------------------|-----1-----|-----2-----|-----3-----|-----4-----|-----5-----|-----6-----|-----7-----|-----8-----|-----9-----|-----10----|-----11----|-----12----|-----13----|-----14----|----15-----|----16-----|-----17----|-----18----|-----19----|-----20----|-----21----|-----22----|-----23----|-----24----|-----25----|-----26----|----27-----|-----28----|-----29----|----30-----|-----31----|-----32----|-----33----|----34-----|-----35----|-----36----|-----37----|-----38----|-----39----|----40-----|-----------|
065_web_collection_003_888_10k_bs4_w4  |75.17,70.34 75.22,71.82 77.63,72.64 77.96,73.68 77.71,73.65 78.94,74.45 77.99,74.74 79.10,75.03 79.49,75.19 79.45,75.07 78.63,75.27 79.14,75.58 79.07,76.18 78.58,76.07 78.46,76.19 78.89,76.07 78.16,75.79 78.31,76.23 77.71,76.25 79.22,76.23
              57.8k   8 samples loss   |1e-4                                                        *1/2    |               #           ##          #           1/4         #   |       ##          *                       *           1/8        |*                       ##                                         |
                       round2          |75.41,70.26 75.50,71.61 78.01,72.81 77.83,73.32 78.31,73.53 78.57,74.07 76.82,73.58 78.17,73.87 79.35,74.98 79.09,74.96 78.92,75.25 78.76,75.57 xx.xx,76.09 78.72,75.95 78.51,76.00 79.05,76.07 78.34,75.73 78.50,76.12 77.46,76.12 79.00,76.18 78.19,76.12 78.38,76.47 78.08,76.15 78.60,76.37 78.77,76.32 
              57.8k   8 samples loss   |1e-4                                                                |                           #1/2        #                        1/4|                   *                       #                      |*1/8                    ##1/16                   1/32               |    1/64                    1/128
065_web_col.._888_10k_train0.95_bs4_w4 |75.27,70.08 76.29,71.26 78.65,73.16 76.98,73.02 76.94,72.87 78.36,73.89 79.87,74.68 78.74,74.80 76.92,74.74 78.49,74.70 78.92,75.52 78.55,75.39 78.37,75.80 78.51,74.34 xx.xx,75.08 78.52,76.16 79.00,75.71 79.29,75.51 78.48,75.58 78.80,76.35 78.44,76.48 78.06,75.96 78.62,76.31 78.44,76.19 79.07,76.45 79.14,76.29 78.42,76.46 78.57,76.55 78.73,76.45 78.59,76.57 78.27,76.28 78.72,76.46 78.48,76.45 78.70,76.50 78.31,76.41 78.82,76.53 79.00,76.52 78.52,76.47 79.05,76.58 78.91,76.53
              57.8k   5% samples loss  |1e-4                    *                                           |   #           *                                   *               |                                            1/2        *          |#                       *                                   *1/4   |                            # 1/8                                  |        1/16                                                       |1/32                    #                       #                   |
                          round2       |74.26,69.28 75.39,71.10 78.20,72.60 76.80,73.11 77.19,73.08 78.16,73.68 79.15,74.52 79.08,74.77 76.22,74.68 77.86,74.68 78.23,75.54 78.52,75.38 78.11,75.76 77.83,74.17 78.52,75.36 77.38,75.26 xx.xx,75.26 78.48,74.86 77.39,74.32 78.04,76.13 77.04,75.25 77.37,75.04 78.55,75.50 77.87,74.34 78.59,75.16 79.31,75.72 77.87,75.91 78.26,76.16 77.46,75.82 77.84,75.99 78.03,75.89 78.13,75.65 76.75,75.84 78.48,75.67 78.06,75.84 78.75,76.16 79.12,76.09 77.42,75.61 78.97,76.35 78.05,76.21 
              57.8k   5% samples loss  |1e-4                                                                |   *           *                                                   |                                                                  |                                                                   |                            *1/2                                  |                                                                   |                         #                       1/4                 |
                          round4       |73.85,66.52 76.35,69.86 77.84,71.71 77.84,72.49 78.87,72.81 79.38,73.48 77.70,74.42 78.95,73.24 76.86,74.19 79.57,75.14 78.57,74.94 77.42,74.82 78.32,74.86 77.38,74.85 78.89,74.02 76.48,74.12 78.78,75.91 77.21,75.16 77.60,75.78 77.81,75.58 79.16,75.49 78.67,75.93 79.16,75.55 78.74,76.11 77.52,75.43 77.69,76.14 78.84,76.40 79.57,76.07 77.16,75.95 78.68,75.78 77.70,75.49 78.37,75.89 78.05,76.23 78.25,76.42 77.65,75.72 78.18,76.35 78.65,76.42 77.61,76.01 78.32,76.23 78.16,76.22 
              57.8k   5% samples loss  |1e-4                                                        *       |               *                       *                           |                               *                       *          |                         1/2        *                       *      |                                                    ##            |         *                                               1/4       |                         *                                           |
                          round5       |73.85,66.52 76.35,69.86 77.84,71.71 77.84,72.49 78.87,72.81 79.38,73.48 77.70,74.42 78.95,73.24 76.86,74.19 79.57,75.14 78.57,74.94 77.42,74.82 78.32,74.86 77.38,74.85 78.89,74.02 76.48,74.12 78.78,75.91 77.21,75.16 77.60,75.78 77.81,75.58 79.16,75.49 78.67,75.93 79.16,75.55 78.74,76.11 77.52,75.43 77.69,76.14 78.84,76.40 79.57,76.07 77.16,75.95 78.68,75.78 77.70,75.49 78.37,75.89 78.05,76.23 78.25,76.42 77.65,75.72 78.18,76.35 78.65,76.42 77.61,76.01 78.32,76.23 78.16,76.22 
              57.8k   5% samples loss  |1e-4                                                        *       |               *                       *                           |                               *                       *          |                         1/2        *                       *      |                                                    ##            |         *                                               1/4       |                         *                                           |
---------------------------------------|-----1-----|-----2-----|-----3-----|-----4-----|-----5-----|-----6-----|-----7-----|-----8-----|-----9-----|-----10----|-----11----|-----12----|-----13----|-----14----|----15-----|----16-----|-----17----|-----18----|-----19----|-----20----|-----21----|-----22----|-----23----|-----24----|-----25----|-----26----|----27-----|-----28----|-----29----|----30-----|-----31----|-----32----|-----33----|----34-----|-----35----|-----36----|-----37----|-----38----|-----39----|----40-----|-----------|
065_web_col.._888_10k_train0.95_bs8_w8 |74.28,69.07 xx.xx,xx.xx 78.40,71.90 78.02,72.96 xx.xx,73.17 77.91,74.62 78.19,74.67 78.26,74.39 78.32,74.88 78.05,73.62 78.20,75.42 78.65,75.21 77.74,75.21 78.33,75.60 xx.xx,75.91 78.48,76.15 79.54,75.27 79.06,76.39 78.02,76.22 78.83,76.20 77.94,76.38 78.94,76.71 78.68,76.51 78.63,76.56 78.88,76.49 78.21,76.43 78.61,76.60 78.21,76.48 78.21,76.47 78.38,76.54 78.36,76.50 78.28,76.51 78.55,76.52 78.31,76.52 78.57,76.60 78.38,76.62 78.68,76.60 78.19,76.64 78.47,76.55 78.43,76.55
              57.8k   8 samples loss   |1e-4                                                                |                                                               *   |                               1/2                     #          |#1/4                                1/8         *           *      |    *           *1/16                   *1/32                    1/|64                  1/128                   1/256                  |1/512                   *1/1024                 1/2048              |
067_888_perspective0.5_train0.95_bs4   |75.81,68.86 75.82,70.51 78.10,71.88 77.02,72.91 77.84,69.14 78.12,73.25 77.05,73.57 76.43,73.15 78.26,72.77 79.12,74.06 78.85,73.83 78.57,73.62 78.47,71.76 77.95,74.81 79.39,74.81 79.24,74.19 77.25,74.03 78.78,74.40 78.40,75.12 78.36,74.68 79.83,73.89 78.60,74.84 78.94,74.93 78.31,75.29 78.84,75.38 78.09,75.26 78.66,75.45 79.80,75.60 78.37,75.75 79.39,76.19 79.01,75.73 78.73,75.61 78.63,76.33 78.02,75.63 79.38,75.07 79.07,76.01 79.03,76.08 79.17,76.37 79.29,76.09 79.20,76.33 
              57.8k   5% samples loss  |1e-4                                                                |                                       *           *               |                               *           *                      |                                    *           *           *      |     1/2                                            ##1/4         |         *           *           *           *                     | *           *1/8        *           *1/16       *           *       |
067_888_perspective0.5_train0.95_bs4   |76.10,68.33 76.04,70.44 78.35,72.18 77.38,73.16 78.23,70.22 77.65,73.58 76.63,73.35 76.98,73.34 77.80,72.01 79.15,73.34 79.21,73.92 78.54,73.90 78.89,72.28 77.84,74.45 79.12,74.68 78.57,73.74 76.78,73.88 78.62,74.43 78.98,75.73 79.19,75.26 80.41,74.69 79.10,75.67 79.14,75.19 77.71,74.93 78.35,75.33 78.10,75.25 79.04,74.86 79.95,75.09 77.77,75.59 79.46,75.96 79.05,75.48 79.36,74.96 77.79,76.01 78.72,75.23 79.83,75.42 79.15,75.94 79.03,75.72 79.60,76.19 79.97,75.36 78.28,76.18 
     round3   57.8k   5% samples loss  |1e-4                                                                |                                       *           *               |                               *                                  |  1/2                               ###         *           *      |                                        *           ##            |         *           *           *                         1/4     | #           *           *           #           ##                  |
---------------------------------------|-----1-----|-----2-----|-----3-----|-----4-----|-----5-----|-----6-----|-----7-----|-----8-----|-----9-----|-----10----|-----11----|-----12----|-----13----|-----14----|----15-----|----16-----|-----17----|-----18----|-----19----|-----20----|-----21----|-----22----|-----23----|-----24----|-----25----|-----26----|----27-----|-----28----|-----29----|----30-----|-----31----|-----32----|-----33----|----34-----|-----35----|-----36----|-----37----|-----38----|-----39----|----40-----|-----------|
069_888__train_mix0.6_1.0              |73.72,67.97 77.14,70.88 78.02,72.20 77.82,71.74 76.85,73.04 78.42,73.37 77.40,74.44 79.28,74.18 79.14,74.41 77.62,74.33 77.46,74.42 78.91,74.58 76.02,75.07 77.56,75.19 76.55,75.27 78.93,75.13 76.92,75.28 78.25,75.06 75.51,74.87 76.01,74.81 78.37,75.78 78.24,75.87 76.52,75.24 78.51,76.00 77.78,76.26 77.02,75.87 78.30,76.75 79.37,75.96 78.67,75.92 78.10,75.98 
              57.8k   5% samples loss  |1e-4                                                                |               *           *                                   *   |                                           *                      |                                                                   |                                                    *           * |
                          round2       |73.94,67.61 76.11,70.66 78.26,70.82 77.69,72.69 77.53,73.37 79.25,74.11 78.54,73.53 77.35,74.00 74.33,72.40 79.08,74.41 79.07,74.70 77.40,74.14 78.33,74.35 79.64,75.09 78.32,74.81 77.41,74.60 79.66,74.63 78.06,74.66 78.64,75.59 78.48,75.36 78.99,74.97 77.41,74.70 79.03,75.50 78.40,75.53 77.63,74.67 78.74,75.46 79.35,75.66 79.29,76.17 77.85,75.68 79.44,75.86 
              57.8k   5% samples loss  |1e-4                                                        *       |                                       *           *               |                   *                                   *          |                                                            *      |                            *1/2        *           #             |         *
070_888__train_mix0.4_1.0              |74.04,67.59 74.73,69.54 78.09,71.94 78.93,72.11 78.19,74.00 78.68,73.88 77.37,74.70 78.55,74.16 76.51,73.37 78.05,74.99 78.89,74.70 78.18,75.10 79.44,75.76 78.29,75.32 77.76,75.78 77.93,75.81 78.39,74.68 78.01,75.69 78.59,75.98 78.78,76.18 
              57.8k   5% samples loss  |1e-4                                *                               |                                                   *               |       *                                                          |            *1/2        *          
                          round2       |72.44,66.26 75.11,69.91 76.72,71.79 76.57,72.84 79.40,73.18 79.18,74.29 78.48,74.96 77.82,74.60 76.43,74.04 74.31,72.53 79.05,75.32 78.82,75.87 77.29,75.26 78.11,75.46 78.59,75.90 76.99,75.33 78.61,75.80 78.59,75.60 78.98,75.83 79.08,75.83
              57.8k   5% samples loss  |1e-4                                            *           *       |                                                   *1/2            |       *                                               *          |  1/4       *           *          
071_888__train_mix0.5_1.0              |74.37,67.94 76.06,70.63 75.94,71.61 78.38,72.39 77.06,73.34 78.53,74.24 78.41,74.48 77.90,74.90 75.43,73.74 77.53,74.53 79.57,74.53 78.18,74.23 76.28,73.91 79.37,74.86 78.02,74.96 78.87,75.19 78.78,75.66 79.41,75.94 78.21,75.99 77.60,75.64 
              57.8k   5% samples loss  |1e-4                                                                |                                                   *               |                                           *           *          |*1/2                                
072_888__train_mix0.7_1.0              |74.97,69.05 75.79,70.22 74.17,69.68 78.01,73.76 77.91,73.55 79.07,74.20 77.43,73.44 77.96,74.43 78.08,75.12 76.22,74.87 76.88,74.37 78.13,74.86 79.28,75.25 75.82,74.91 78.28,75.30 75.41,74.51 78.21,74.97 77.91,74.83 76.65,75.14 77.53,75.20 
              57.8k   5% samples loss  |1e-4                                                        *       |                                                                   |       *                                                          |                                    
---------------------------------------|-----1-----|-----2-----|-----3-----|-----4-----|-----5-----|-----6-----|-----7-----|-----8-----|-----9-----|-----10----|-----11----|-----12----|-----13----|-----14----|----15-----|----16-----|-----17----|-----18----|-----19----|-----20----|-----21----|-----22----|-----23----|-----24----|-----25----|-----26----|----27-----|-----28----|-----29----|----30-----|-----31----|-----32----|-----33----|----34-----|-----35----|-----36----|-----37----|-----38----|-----39----|----40-----|-----------|
068_888_shiftscale_train0.95           |76.08,68.30 72.39,69.03 76.68,72.14 76.36,72.74 79.24,73.08 77.95,73.69 78.79,73.87 76.90,73.38 76.93,73.60 78.64,74.74 79.39,74.55 78.04,75.07 79.10,74.97 79.73,75.49 80.27,75.96 79.51,75.62 77.76,75.54 78.34,75.55 78.39,76.02 79.67,76.06 79.64,75.35 79.45,76.12 77.56,75.28 78.95,75.64 79.43,75.88 78.51,75.51 78.21,75.39 79.46,76.32 78.73,75.76 79.82,76.61 79.67,76.22 79.52,75.93 78.67,76.02 79.11,75.92 78.92,75.60 79.73,76.45 79.39,76.44 78.92,76.33 78.65,76.15 79.07,76.18 
              57.8k   5% samples loss  |1e-4                                            *                   |                                                   *           *1/2|       *           #           ##          #                      |                        #           #           *                  |                *                                   * 1/4         |         ##          ##          *                       *         |             ## 1/8      #                                           |
068_888_shiftscale_train0.95  round3   |73.67,68.62 76.53,71.32 76.35,71.83 76.58,73.14 79.94,74.22 78.55,74.34 77.48,74.16 78.90,75.04 78.17,74.27 78.54,75.49 78.50,75.10 78.13,75.10 78.18,75.27 78.09,74.61 77.75,74.63 75.70,74.88 78.39,75.78 77.71,74.71 79.05,75.39 80.36,76.37 79.31,75.88 78.80,75.92 79.79,75.81 78.31,76.09 78.82,75.78 79.77,76.39 xx.xx,76.01 xx.xx,76.57 78.83,76.13 79.10,76.63 79.59,76.22 79.83,76.68 79.15,76.41 79.36,76.50 79.04,76.49 79.55,76.60 79.41,76.46 79.40,76.38 79.57,76.63 79.55,76.67 
              57.8k   5% samples loss  |1e-4                                            ##                  |                                                                   |                                                                  |            *           ###         * 1/2                   ##     |                            ##                                    |         *           ##          ### 1/4     *           *         |             ##          #           #           ##          ## 1/8  |
068_888_shiftscale_train0.95  round4   |73.67,68.62 76.53,71.32 76.35,71.83 76.58,73.14 79.94,74.22 78.55,74.34 77.48,74.16 78.90,75.04 78.17,74.27 78.54,75.49 78.50,75.10 78.13,75.10 78.18,75.27 78.09,74.61 77.75,74.63 75.70,74.88 78.39,75.78 77.71,74.71 79.05,75.39 80.36,76.37 79.31,75.88 78.80,75.92 79.79,75.81 78.31,76.09 78.82,75.78 79.77,76.39 xx.xx,76.01 xx.xx,76.57 78.83,76.13 79.10,76.63 79.59,76.22 79.83,76.68 79.15,76.41 79.36,76.50 79.04,76.49 79.55,76.60 79.41,76.46 79.40,76.38 79.57,76.63 79.55,76.67 
              57.8k   5% samples loss  |1e-4                                            ##                  |                                                                   |                                                                  |            *           ###         * 1/2                   ##     |                            ##                                    |         *           ##          ### 1/4     *           *         |             ##          #           #           ##          ## 1/8  |
068_888_shiftscale_train0.95 seed78    |74.24,67.31 73.50,68.61 77.95,72.20 77.83,72.69 79.17,73.90 79.23,73.83 77.89,73.40 77.91,74.39 79.33,74.62 76.63,73.86 78.39,74.68 78.71,74.56 77.79,74.58 77.65,73.81 79.71,74.59 77.83,75.23 79.49,75.13 79.57,75.75 78.85,75.78 78.25,75.94 77.69,75.21 78.63,75.83 79.08,75.93 78.63,75.83 78.79,76.00 77.80,75.69 78.69,75.74 79.02,76.09 77.75,75.64 79.70,76.31 79.55,76.19 78.32,75.52 78.50,76.12 78.79,76.57 78.75,76.23 79.20,76.42 79.19,76.09 79.55,76.68 79.37,76.51 79.23,76.71 
              57.8k   5% samples loss  |1e-4                                            *           *       |                           *                                       |                               #             1/2       *          |#                                                           *      |                                                    *             |         #           #                       * 1/4       *         |             *           *           # 1/8       *           *       |
068_888_shiftscale_train0.95 seed90    |74.75,69.40 76.64,71.20 78.22,72.81 75.75,72.73 76.97,73.59 77.13,74.07 77.03,74.10 79.31,74.25 76.43,74.54 78.71,74.41 79.10,75.17 78.56,74.99 78.96,75.77 77.37,75.65 78.80,75.98 78.44,75.53 76.61,74.60 77.77,75.03 78.72,75.33 80.18,75.49 78.53,75.57 77.31,75.36 79.68,75.82 79.39,75.25 78.94,75.42 78.30,76.01 79.22,76.12 78.33,75.26 79.01,75.56 78.46,76.00 79.53,76.65 78.76,76.20 79.41,76.51 79.61,76.54 78.95,76.52 78.83,76.10 80.18,76.18 79.88,76.35 79.13,76.63 79.24,76.51 
              57.8k   5% samples loss  |1e-4                                                                |               *                                   *               |                                                       *          |                        ##                                  #      |    *                                   *                       * |           1/2       *                       *           #         |                         ##          # 1/4       *           *       |
068_888_shiftscale_train0.95 seed12    |72.54,66.75 74.73,69.68 77.86,71.48 78.05,72.91 79.26,72.89 79.68,73.89 78.10,73.97 78.78,74.12 80.18,74.86 79.08,74.29 79.03,74.79 78.27,74.97 79.31,75.45 77.48,74.79 79.10,75.34 78.36,75.03 77.85,74.88 77.68,75.20 76.91,74.78 78.31,75.79 80.04,75.93 79.49,75.92 78.61,75.25 78.99,75.98 77.99,75.11 79.70,76.02 79.23,75.89 79.96,75.61 79.07,75.87 79.71,75.75 79.25,75.82 79.13,76.10 78.59,76.52 78.63,76.20 79.75,76.65 79.38,75.99 79.60,76.85 79.63,76.58 79.61,76.56 78.75,76.49 
              57.8k   5% samples loss  |1e-4                                            *           #       |                           ##          *           *               |       *                       *                                  |                                    ##          *                  |                            #           *           ##          * |         #           *           * 1/2                             | #           *           # 1/4       #           #                   |
068_888_shiftscale_train0.95 seed24    |71.56,68.28 76.39,71.37 77.28,72.13 78.97,73.28 77.94,73.65 77.60,72.36 77.22,73.44 79.36,74.57 76.82,74.30 79.66,74.36 77.91,73.83 80.21,74.52 77.75,74.68 77.60,75.07 77.79,75.13 77.57,75.11 78.44,75.27 76.54,74.98 77.12,75.60 77.49,75.10 76.50,75.15 79.39,76.25 79.13,75.63 78.40,75.61 78.96,76.26 78.75,76.33 79.67,76.08 79.66,75.95 79.08,76.29 78.79,76.46 79.74,76.15 79.52,76.59 78.51,76.14 78.94,76.27 79.29,76.00 79.84,76.29 79.79,76.72 80.25,76.64 79.10,76.42 79.47,76.46 
              57.8k   5% samples loss  |1e-4                                                                |               *                       #                           |                                                                  |                                                * 1/2       *      |                                        #           #           * |                     #           *                                 | *           #           # 1/4       ##          *           *       |
068_888_shiftscale_train0.95 seed35    |76.58,69.74 77.40,71.68 78.69,72.45 76.64,72.79 75.59,73.37 76.87,73.77 76.09,73.16 78.10,74.45 74.90,73.01 77.66,74.83 77.73,75.12 77.07,74.24 77.32,75.12 78.33,75.06 76.69,74.95 80.00,76.30 77.98,75.71 77.57,75.62 79.60,75.65 79.72,75.87 79.64,76.15 78.24,75.83 79.80,76.15 
              57.8k   5% samples loss  |1e-4                                                                |               *                       #                           |                     1/2                                          |            #           #           #                       #      |
068_888_shiftscale_train0.95 seed46    |73.93,67.76 74.65,69.88 76.39,70.99 76.01,71.76 77.26,73.37 78.10,74.19 79.15,72.65 77.76,73.39 79.57,74.01 78.21,74.31 78.05,74.51 78.04,74.73 78.59,74.52 77.98,75.09 79.35,76.00 79.46,75.97 77.76,75.77 79.88,75.80 79.29,76.24 78.91,76.26 79.32,76.41 78.13,75.88 79.33,76.29 79.11,76.26 78.90,76.22 79.37,76.21 79.39,76.37 79.35,76.60 79.44,76.47 78.94,76.52 79.10,76.49 79.16,76.62 78.55,76.35 79.06,76.32 79.33,76.71 79.42,76.61 79.43,76.58 79.33,76.65 79.19,76.62 79.72,76.88 
              57.8k   5% samples loss  |1e-4                                                                |   *                       *                                       |                     1/2       *           *                      |#           * 1/4                   *                       *      |    *                       * 1/8       *           *           * |                     *           *                       *         | *           *           * 1/4       *           *           # 1/8   |
068_888_shiftscale_train0.95 seed56    |74.50,67.76 xx.xx,71.07 78.04,72.44 76.18,72.38 78.30,73.23 77.04,72.98 xx.xx,74.49 78.76,74.86 79.75,74.61 79.09,74.42 77.76,74.89 xx.xx,75.38 79.16,75.12 78.66,74.38 77.71,75.26 78.26,74.99 78.26,75.44 79.56,75.64 78.54,75.85 79.32,75.17 79.71,75.83 79.26,75.84 78.95,76.18 77.59,75.96 79.63,76.34 78.86,76.29 79.56,76.56 78.90,76.48 78.71,76.22 xx.xx,76.21 79.99,76.77 78.93,76.12 79.21,76.70 79.34,76.44 xx.xx,76.62 79.55,76.54 79.46,76.93 79.69,76.52 79.80,76.79 78.84,76.58 
              57.8k   5% samples loss  |1e-4                                                                |                           #           *                           |       *                                                          |*           *                       # 1/2       *                  |                #                       *                         |                     ## 1/4                              *         | *           *           *           #           #           #       |
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



best results @ 20210414

053_reproduce_77_mAP_uint8_train0.6/best-checkpoint-001epoch.bin
private50
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 96.58% | 96.52% | 96.41% | 96.07% | 95.44% | 94.10% | 88.80% | 75.04% | 42.83% |  4.33% | 78.61% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
web-collection-001-002
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 92.10% | 91.78% | 91.23% | 90.31% | 88.63% | 84.74% | 75.62% | 56.18% | 24.91% |  1.52% | 69.70% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+

065_reproduce_77_mAP_uint8_aug_web_collection_003_888_10k_virtual_epoch/best-checkpoint-019epoch.bin
private50
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 96.52% | 96.44% | 96.29% | 95.94% | 95.53% | 94.30% | 89.05% | 76.02% | 45.85% |  6.27% | 79.22% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
web-collection-001-002
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 95.40% | 95.29% | 95.00% | 94.52% | 93.40% | 91.09% | 85.07% | 69.39% | 38.75% |  4.35% | 76.23% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
---------------------------------------------------------------------------------------------------------



duplicated

jared-murray-NSuufgf-BME-unsplash	photo-1536099629323-44806c1ea264
will-truettner-g5qWYuTKkok-unsplash	photo-1536685712909-6ac3633e0812






https://github.com/wk910930/diagnosing-object-detectors
	most confident false positives
	shows detection confidences of a subset of objects, from most to least confident
	small iou matches
ratio in different IoU slots
missed objects
misclassified

deduplicate image in dataset

https://voxel51.com/docs/fiftyone/getting_started/install.html#installing-fiftyone
https://github.com/voxel51/fiftyone
crop images
shadow aug


1ce9fe3b4e7c1233abcbd1e5fc27ea4a_00001_05
	水印
0880a79779020b53450841c0e58fde90
	水印
*35152_00001*.*
*35152_00002*.*
*2360cc*.*
*a82626*.*
72718_00003
	missed one car in the dark on the right side
911e1c_00002
	高架支架
drone-5946395.jpg
	duplicated
2334722_00002
	missing stopped dark cars

9f2f7c4d8
	incorrect bus size (right bottom)
kyler-boone-GRu2e_Z01-o-unsplash
	2 or more building top as cars
		split num: 31, 45

3004afc7e5a119f736a5d26de4fdad19_00001_41.jpg
	false positive
3004afc7e5a119f736a5d26de4fdad19_00001.jpg
	missing truck, car, car with shadow
sienna-xxUdssTb1fI-unsplash_03.jpg
	missing car, false positive
79dd1975d565bef19de0e717d3486a53_00001_00.jpg
	missing truck
29212b2af422ba42a0c469cc8b918ac3_00003_60.jpg
	roof
vjshi.com_2021-01-07_e51beb279a5051feaa356aa3eedac837_00003_00.jpg
	missing small car



wangxinyu0413
c361d355462bc*____00002
	no changes

zhaoqiang0413
1ea0f49856cc2d3*_____00002
1de990a810*_____00001
fbc81676a27*_____00004
	no changes





blur is harmful...
	?

try no blur only
then larger training set
then larger batch size and suitable lr
better crop setting
more data with enough diversity
	shadow
more geometry transformation
different prob values
the order of augmentation
virtual epoch
	to verify the effect of multiple workers on dataloader
	or train with smaller training set

target
	train0.95 78.2%

error analyse
	best results
	worst results


dataset exploration
	random scaling might improve mAP on scales that other than 60 pixel
	very inbalanced groups
		1000 : 1 or even worse
	balanced sample in one batch
	for most common cars
		3 major resolution in dataset
			27 * 11
			47 * 20
			68 * 27
		1 major aspect ratio
			2.4 : 1


============== DONE ==============
小的透视投影变形
virtual epoch查看训练过程中的mAP
用不同的seed分割的数据集做训练
胶片噪声加JPEG压缩噪声
随机缩放平移
better mAP evaluation
better dataset mixing
更大的训练集
筛选测试集图片，以及测试集多样性不充分以及和训练集重叠的问题
	测试集不包含模糊不清的低分辨率图片和道路外停放的车辆
数据集mmdetection可视化，确定角度是否有问题

==================================
公交车？
  没有缩放的情况下确实还是比较多的底置信度和漏检

last layer tuning or all layers
efficientdet to reid？one stage
2 stage faster rcnn
  with resnest

tensorboard loss mAP
r3det-dcl


TODO:
eagle合并代码
eagle根据车辆像素尺寸在检测前缩放图片
添加跟踪标注模式
验证virtual epoch以及loader worker效果

标注：
平滑轨迹
筛除不符合要求的预标注
剔除长宽突变轨迹
剔除短轨迹

训练：
检查sparse rcnn底层输入分辨率
裁剪尺寸合理化
更大的batch size与合适的学习率
贝叶斯调超参

集成：
weighted box fusion ensemble
Stochastic Weights Averaging (SWA)

数据采样：
记录修改过的预标注的数据，基于修改iou的crop采样
平衡batch中的数据
划分样本，先少后多，先易后难，batch中用类似样本
分层抽样
more negative sample

数据分析：
length/width ratio of false positive
locatiion distribution
gps distribution
country distribution
Model Card Toolkit

增广：
自动增广
训练时在线random crop
在线旋转切图以及扭曲图像
利用gan做车辆分布增广
比较人为标记间的差异

==================================





GridDistortion 
OpticalDistortion 
ImageCompression 
ISONoise 
JpegCompression 
MedianBlur 
MotionBlur 
MultiplicativeNoise 
Posterize 
RandomShadow 
RandomToneCurve
Sharpen 
Superpixels 
RandomGamma

ShiftScaleRotate
ElasticTransform

RandomCropNearBBox
RandomSizedBBoxSafeCrop

Build dataset:
   5000	0009_dataset_20200901M2_20200907_1202_200m_fixed_768_768_obb
   5000	0010_dataset_20200901M2_20200907_1202_200m_fixed_1536_768_obb
    250	0011_dataset_20200901M2_20200907_1202_200m_fixed_768_768_obb_bus
    250	0012_dataset_20200901M2_20200907_1202_200m_fixed_1536_768_obb_bus
   3124	0013_dataset_tongji_011_768_768_obb
   5000	0014_dataset_20200901M2_20200903_1205_250m_fixed_768_768_obb
   5000	0015_dataset_20200901M2_20200907_1104_200m_fixed_768_768_obb
   4940	0016_dataset_ysq1_768_768_obb
   4940	0017_dataset_ysq1_1440_768_obb
   8000	0018_syq4_dataset_768_768_obb_bus
   8000	0019_gm7_dataset_768_768_obb_bus
Training set:     39603
Test set:          9901


045_train0.95_isonoise_bs8
	exception...

Learning Rate: 0.000100
<class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
Fitter prepared. Device is cuda:0

2021-04-10T03:32:11.425500
LR: 0.0001
[RESULT]: Train. Epoch: 0, summary_loss: 2.13737, time: 40.3 mins                            
[RESULT]: Val. Epoch: 0, summary_loss: 0.14708, time: 0.7 mins                        

2021-04-10T04:13:11.105061
LR: 0.0001
/home/me/1TSSD/maliang/efficientdet-pytorch/_models/045_reproduce_77_mAP_uint8_aug_train0.95_isonoise_round1_bs8/best-checkpoint-000epoch.bin
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 95.39% | 95.28% | 95.07% | 94.69% | 94.15% | 92.02% | 85.48% | 69.29% | 37.74% |  3.56% | 76.27% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
/home/me/1TSSD/maliang/efficientdet-pytorch/_models/045_reproduce_77_mAP_uint8_aug_train0.95_isonoise_round1_bs8/best-checkpoint-000epoch.bin
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 92.01% | 91.60% | 91.01% | 89.76% | 87.76% | 82.79% | 71.84% | 50.56% | 20.01% |  0.95% | 67.83% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
Traceback (most recent call last):: 0.12220, time: 36.4 mins remaining: 3.9 mins             
  File "train2.py", line 646, in <module>
    run_training(net, output_path, logger)
  File "train2.py", line 521, in run_training
    fitter.fit(train_loader, val_loader)
  File "train2.py", line 329, in fit
    summary_loss = self.train_one_epoch(train_loader)
  File "train2.py", line 402, in train_one_epoch
    images = torch.stack(images)
TypeError: expected Tensor as element 1 in argument 0, but got numpy.ndarray





Found non-tensor inputs!mary_loss: 0.10660, time: 1.5 mins remaining: 10.1 mins            
(array([[[106,  99, 107],
        [106,  99, 107],
        [126, 119, 126],
        ...,
        [101,  93,  56],
        [101,  93,  56],
        [101,  93,  56]],

       [[109, 102, 110],
        [110, 103, 110],
        [130, 123, 130],
        ...,
        [101,  93,  56],
        [101,  93,  56],
        [101,  93,  56]],

       [[112, 107, 113],
        [115, 110, 116],
        [134, 129, 135],
        ...,
        [101,  93,  56],
        [101,  93,  56],
        [101,  93,  56]],

       ...,

       [[ 60,  71,  99],
        [ 64,  75, 103],
        [ 69,  80, 108],
        ...,
        [ 43,  56,  73],
        [ 39,  52,  69],
        [ 36,  49,  66]],

       [[ 47,  58,  86],
        [ 49,  60,  88],
        [ 49,  60,  88],
        ...,
        [ 43,  56,  73],
        [ 39,  52,  69],
        [ 36,  49,  66]],

       [[ 47,  58,  86],
        [ 47,  58,  86],
        [ 46,  57,  85],
        ...,
        [ 44,  57,  74],
        [ 40,  53,  70],
        [ 37,  50,  67]]], dtype=uint8), tensor([[[  0,   0,   0,  ..., 119, 119, 118],
         [  0,   0,   0,  ..., 118, 118, 117],
         [  0,   0,   0,  ..., 118, 117, 115],
         ...,
         [  0,   0,   0,  ..., 149, 146, 127],
         [  0,   0,   0,  ..., 125, 112, 100],
         [  0,   0,   0,  ..., 114,  97,  90]],

        [[  0,   0,   0,  ..., 118, 118, 119],
         [  0,   0,   0,  ..., 117, 117, 118],
         [  0,   0,   0,  ..., 120, 119, 119],
         ...,
         [  0,   0,   0,  ..., 146, 143, 125],
         [  0,   0,   0,  ..., 122, 109,  98],
         [  0,   0,   0,  ..., 111,  95,  88]],

        [[  0,   0,   0,  ..., 115, 115, 117],
         [  0,   0,   0,  ..., 114, 114, 115],
         [  0,   0,   0,  ..., 114, 113, 113],
         ...,
         [  0,   0,   0,  ..., 130, 131, 113],
         [  0,   0,   0,  ..., 106,  95,  88],
         [  0,   0,   0,  ...,  94,  81,  77]]], dtype=torch.uint8), tensor([[[ 5,  5,  5,  ...,  5,  5,  5],
         [ 5,  5,  5,  ...,  5,  5,  5],
         [ 5,  5,  5,  ...,  5,  5,  5],
         ...,
         [ 5,  5,  5,  ..., 54,  5,  5],
         [ 5,  5,  5,  ..., 46,  5,  5],
         [ 5,  5,  5,  ..., 44,  5,  5]],

        [[ 5,  5,  5,  ...,  5,  5,  5],
         [ 5,  5,  5,  ...,  5,  5,  5],
         [ 5,  5,  5,  ...,  5,  5,  5],
         ...,
         [ 5,  5,  5,  ..., 63,  5,  5],
         [ 5,  5,  5,  ..., 58,  5,  5],
         [ 5,  5,  5,  ..., 53,  5,  5]],

        [[ 5,  5,  5,  ...,  5,  5,  5],
         [ 5,  5,  5,  ...,  5,  5,  5],
         [ 5,  5,  5,  ...,  5,  5,  5],
         ...,
         [ 5,  5,  5,  ..., 47,  5,  5],
         [ 5,  5,  5,  ..., 41,  5,  5],
         [ 5,  5,  5,  ..., 39,  5,  5]]], dtype=torch.uint8), tensor([[[2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         ...,
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2]],

        [[2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         ...,
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2]],

        [[2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         ...,
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2],
         [2, 2, 2,  ..., 2, 2, 2]]], dtype=torch.uint8))
Traceback (most recent call last):
  File "train2.py", line 410, in train_one_epoch
    images = torch.stack(images)
TypeError: expected Tensor as element 0 in argument 0, but got numpy.ndarray

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train2.py", line 700, in <module>
    run_training(net, output_path, logger)
  File "train2.py", line 564, in run_training
    fitter.fit(virtual_train_loader, val_loader)
  File "train2.py", line 336, in fit
    summary_loss = self.train_one_epoch(train_loader)
  File "train2.py", line 415, in train_one_epoch
    images = torch.stack(images)
RuntimeError: stack expects each tensor to be equal size, but got [768, 768, 3] at entry 0 and [3, 768, 768] at entry 1




(Pdb) print(train_loader.data_loader.dataset.img_names[21827])
../../_datasets/0020_web-collection-003_1184_768_768_obb/vjshi.com_2020-09-09_c57176174ffb24a2189cd5e958e90cac_00004_09.jpg
(Pdb) print(step)
1062

(Pdb) images = [images[0],images[1],torch.from_numpy(images[2].transpose(2, 0, 1)),images[3]]
(Pdb) images = torch.stack(images)
{'boxes': tensor([[722.8071, 743.6644, 768.0000, 768.0000, 270.0000]]), 'labels': tensor([1]), 'image_id': tensor([21827])},
(Pdb) print(train_loader.data_loader.dataset.img_names[21827])
../../_datasets/0020_web-collection-003_1184_768_768_obb/vjshi.com_2020-09-09_c57176174ffb24a2189cd5e958e90cac_00004_09.jpg

Traceback (most recent call last):
  File "train2.py", line 698, in <module>
    run_training(net, output_path, logger)
  File "train2.py", line 562, in run_training
    fitter.fit(virtual_train_loader, val_loader)
  File "train2.py", line 336, in fit
    summary_loss = self.train_one_epoch(train_loader)
  File "train2.py", line 415, in train_one_epoch
    images = images.to(self.device).float() / 255.0
  File "/home/me/anaconda3/envs/efficientdet-pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/me/1TSSD/efficientdet-pytorch/effdet/bench.py", line 94, in forward
    x.shape[0], target['bbox'], target['cls'])
  File "/home/me/1TSSD/efficientdet-pytorch/effdet/anchors.py", line 411, in batch_label_anchors
    anchor_box_list, BoxList(gt_boxes[i]), gt_classes[i])
  File "/home/me/1TSSD/efficientdet-pytorch/effdet/object_detection/target_assigner.py", line 144, in assign
    match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes, anchors)
  File "/home/me/1TSSD/efficientdet-pytorch/effdet/object_detection/region_similarity_calculator.py", line 101, in compare
    return iou(boxlist1, boxlist2)
  File "/home/me/1TSSD/efficientdet-pytorch/effdet/object_detection/region_similarity_calculator.py", line 69, in iou
    intersections = intersection(boxlist1, boxlist2)
  File "/home/me/1TSSD/efficientdet-pytorch/effdet/object_detection/region_similarity_calculator.py", line 48, in intersection
    y_min1, x_min1, y_max1, x_max1 = boxlist1.boxes().chunk(4, dim=1)
ValueError: not enough values to unpack (expected 4, got 3)





======================================================================================================
023
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 96.39% | 96.32% | 96.06% | 95.61% | 95.21% | 92.99% | 86.79% | 70.96% | 40.30% |  5.13% | 77.58% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 92.29% | 91.95% | 91.34% | 90.17% | 88.33% | 83.74% | 73.50% | 53.09% | 23.06% |  1.48% | 68.89% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+

053
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 96.60% | 96.51% | 96.37% | 96.08% | 95.43% | 93.99% | 88.78% | 74.79% | 42.64% |  4.30% | 78.55% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 92.06% | 91.71% | 91.11% | 90.23% | 88.53% | 84.46% | 75.52% | 55.59% | 25.07% |  1.49% | 69.58% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+

065
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 96.52% | 96.44% | 96.29% | 95.94% | 95.53% | 94.39% | 89.08% | 75.97% | 45.79% |  6.40% | 79.23% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| 95.40% | 95.31% | 95.01% | 94.54% | 93.48% | 91.11% | 85.05% | 69.20% | 38.79% |  4.24% | 76.21% |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+


re-create my test sets...
    and resize to vehicle width to 30 pixels

3090
	driver, cuda and pytorch
sudo apt install -y nvidia-driver-460
sudo apt install cuda-11-0
sudo dpkg -i '/home/me/1TSSD/_soft_download/libcudnn8_8.0.5.39-1+cuda11.0_amd64.deb' 
sudo dpkg -i '/home/me/1TSSD/_soft_download/libcudnn8-dev_8.0.5.39-1+cuda11.0_amd64.deb' 
pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html

torch                     1.8.0.dev20210208+cu110          pypi_0    pypi
torchaudio                0.8.0.dev20210208                pypi_0    pypi
torchvision               0.9.0.dev20210211+cu110          pypi_0    pypi

======================================================================================================
todo

efficientnet v2 + object detection head
* Chris Hughes has put together a great example of training w/ `timm` EfficientNetV2 backbones and the latest versions of the EfficientDet models here
  * [Medium blog post](https://medium.com/data-science-at-microsoft/training-efficientdet-on-custom-data-with-pytorch-lightning-using-an-efficientnetv2-backbone-1cdf3bd7921f)

