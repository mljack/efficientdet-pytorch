Home workstation @ 2022

https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730
https://blog.neater-hut.com/virtual-epochs-for-pytorch.html


from tensorboardX import SummaryWriter



(pytorch2022) F:\ws>python -m efficientdet_pytorch_win64.infer det_test 25 _models/model-092-best-checkpoint-035epoch.bin obb


recheck old 2021 training dataset


(pytorch2022) F:\ws>python -m efficientdet_pytorch_win64.infer F:\dataset-recheck\0019_gm7_dataset_768_768_obb_bus -1 _models/model-092-best-checkpoint-035epoch.bin obb


2021 - Bootstrap Your Object Detector via Mixed Training
	mix pseudo labels with ground truth labels
		based on IoU
			applying an exponential moving average (EMA) model on the input images without any data augmentation except for scale jitter
				to read
					2021 - A simple semi-supervised learning framework for object detection. ICLR.
					2020 - Self-training with noisy student improves imagenet classification
		i guess it needs to wait until its detector is good enough,
			or only use high confident predictions for pseudo
			otherwise false positives will pollute gt labels.
	use strong augmentation on confidence samples.
		so augmentation is growning stronger on training
			this reduces the difficulty in training/optimization process
				intinct: only learning the advanced until you have learnt the basics.


https://github.com/albumentations-team/albumentations/releases
	A.Rotate and A.ShiftScaleRotate now support new rotation method for bounding boxes, ellipse. (#1203 by @victor1cea)
	Added position argument to PadIfNeeded (#933 by @yisaienkov)
		Possible values: center top_left, top_right, bottom_left, bottom_right, with center being the default value.


https://github.com/mljack/albumentations_mosaic
	https://github.com/albumentations-team/albumentations/issues/340
	https://github.com/albumentations-team/albumentations/pull/1147
	https://github.com/i-aki-y/albumentations/tree/add-mosaic-aug


(pytorch2022) F:\ws>python -m efficientdet_pytorch_win64.infer F:\ws\CyTrafficEditor2\Source\CyTrafficEditor\CyTraffic\data\videos\Munich3K 25 _models/model-092-best-checkpoint-035epoch.bin obb 17

开始标注 Munich3K OBB
	误检主要源于相近形状物体
		主要对物体形状和边缘敏感
		缺乏对位置语义
			大部分车辆位于道路之上
			屋顶位置的窗户
			尺寸形状相似的房顶和屋檐
		缺乏对于尺度的理解
			同一图片中把不同尺度的物体检测成车
		缺乏对阴影的理解
			阴影代表高度与凹凸
				与周围物体的高度差相关
			阴影形状尺寸刚好和车辆类似
			阴影造成车辆尺寸检测精度降低
			地上文字没有阴影也就表明没有高出地面
			隧道口
			桥梁的一段
			长宽接近于2：1的花坛
		密集排列车辆相互干扰
		难以区分白色集装箱与拖挂车以及路边的垃圾箱
	漏检主要源于遮挡、暗光以及稀有车辆和卡车货物多样化
		阴影暗处有高光或浅色头灯很可能是车

10420	VAID_aabb		road
8944	DroneVehicle		road
1604	UAV-ROD			mostly road
1923	VEDAI			country side
688	VSAI			mostly road
	Munich3K
	Estonia_Aerial_2019
	cofga
	linz


tune the lr schedule

trainning schedule for object detection
	mmdetection 1x 2x
		Why 1x, 2x-Schedule steps at 8,11 & 16, 22 Epoch
			https://github.com/open-mmlab/mmdetection/issues/5296
FPN uses a simple schedule: [60k, 80k] iterations (I write schedules including termination).
Mask R-CNN (ICCV 2017) adds one more step for human pose estimation: [60k, 80k, 90k] iterations.
Detectron popularizes the schedule.

1x: For minibatch size 16, this schedule starts at a LR of 0.02 and is decreased by a factor of * 0.1 after 60k
	and 80k iterations and finally terminates at 90k iterations. This schedules results in 12.17 epochs over
	the 118,287 images in coco_2014_train union coco_2014_valminusminival (or equivalently, coco_2017_train).
2x: Twice as long as the 1x schedule with the LR change points scaled proportionally.
Mask R-CNN (arXiv v3) uses the 2x schedule as the updated baseline for instance segmentation and object detection.

MMDetection basically uses epoch-based training.
[60k, 80k, 90k] iterations correspond to [8, 11, 12] epochs.
60000*16/118287 = 8.1
80000*16/118287 = 10.8
90000*16/118287 = 12.2

2019 - MMDetection: Open MMLab Detection Toolbox and Benchmark
COCO 2017
Images are resized to a maximum scale of 1333 × 800,without changing the aspect ratio.
We use 8 V100 GPUs for training with a total batch size of 16 (2 images per GPU) and a single V100 GPU for inference.
The training schedule is the same as Detectron [10].
	“1x” and “2x” means 12 epochs and 24 epochs respectively. 
	“20e” is adopted in cascade models, which denotes 20 epochs.

093 40epochs lr下降过慢，未充分收敛

bs4, 1w samples, lr1e-4, 40epochs  (087): 88.21 0.37	lr下降适当，epoch过少但还未充分收敛
bs8, 2w samples, lr2e-4, 80epochs  (097): 88.51 0.23	lr下降过快，新增DroneVehicle
bs8, 2w samples, lr2e-4, 40epochs  (092): 88.49 0.35	提高一倍训练量，新增VSAI数据集，首次lr下降过早(13epoch)
bs8, 2w samples, lr2e-4, 80epochs  (095): 88.20 0.17	提高一倍训练量，新增VAID_aabb数据集，多训练40epochs，首次lr下降过早(15epoch)
bs8, 2w samples, lr2e-4, 80epochs  (098): 88.33 0.10	提高一倍训练量，新增VEDIA数据集，多训练40epochs，首次lr下降过早(15epoch)
bs8, 2w samples, lr2e-4, 60epochs(z0007): 88.93 0.17	多训练20epochs, 新增4个数据集(不包含VAID_aabb)，lr下降适当(24epoch)
bs8, 2w samples, lr2e-4, 60epochs(z0009): 89.11 0.13	多训练20epochs, 新增5个数据集，lr下降适当(27epoch)
bs8, 2w samples, lr2e-4, 60epochs(z0010): 		多训练20epochs, 新增4个数据集(不包含VAID_aabb), 90% rot aug，导致lr下降过快

再提高一倍起始学习率，来提高收敛速度
3090 lr schedule
	bs8, 2w samples, lr2e-4,  80epochs, MultiStepLR[30,40,50,60,70]       (z0011): 88.75_0.09  lr调整比较适当，但rotaug0.9可能造成训练与测试/验证差异过大，仍未达到最佳水平
	bs8, 2w samples, lr4e-4,  80epochs, MultiStepLR[15,30,40,50,60,70]    (z0014): 88.54_0.10  恢复使用rotaug0.5，使用包括新的5个数据集的训练集，在前15个epoch使用更高学习率，mAP低于预期，第31epoch后稍有过拟合
	bs8, 2w samples, lr4e-4, 105epochs, MultiStepLR[15,30,45,60,75,90]    (z0016): 88.67_0.14  每个step 15个epoch，第31epoch后稍有过拟合，过拟合程度比z0014稍大
	bs8, 4w samples, lr4e-4, 105epochs, MultiStepLR[15,30,45,60,75,90]   (zb0017): 88.21_0.14  4w sample，第16epoch后稍有过拟合，而后过拟合扩大，最终比z0016稍大，但不是很大(training loss:0.0906, val loss:0.08525)

2080ti lr schedule
	bs4, 5k samples, lr2e-4,  80epochs, MultiStepLR[15,30,40,50,60,70]    (z0013): 87.59_0.16  新的5个数据集，整体训练量不够，结果噪声水平还比较高
	bs4, 1w samples, lr2e-4,  80epochs, MultiStepLR[15,30,40,50,60,70]    (z0015): 88.17_0.18  新的5个数据集，整体训练量增加一倍,效果有所提升但有余地，新的5个数据集的问题？
	bs4, 2w samples, lr2e-4,  80epochs, MultiStepLR[15,30,40,50,60,70]    (z0017): 88.52_0.15  新的5个数据集，整体训练量再增加一倍到2w，mAP稍有提高，开始有一点过拟合
	bs4, 1w samples, lr4e-4,  80epochs, MultiStepLR[15,30,40,50,60,70]    (z0019): 88.34_0.13  不使用新的5个数据集，lr再增加一倍到0.0004
	bs4, 1w samples, lr4e-4,  80epochs, MultiStepLR[15,30,40,50,60,70]    (z0018): 88.43_0.15  不使用新的5个数据集，lr不变，为0.0002
	bs4, 1w samples, lr1e-4,  80epochs, MultiStepLR[30,40,50,60]         (za0020): 88.08_0.15  不使用新的5个数据集，lr起始为0.0001，第一个step在30，中间每10一个step，最后的step跑20epochs



使用训练时模型进行推理，单一显卡完成训练和验证
val使用rot aug

obb iou计算mAP(done)

2020 - A Simple Semi-Supervised Learning Framework for Object Detection
	deploys highly confident pseudo labels of localized objects from an unlabeled image
		and updates the model by enforcing consistency via strong augmentations.

	Since the training of the object detector is quite involved,
	we stay with the default learning settings for all our experiments
	other than the learning schedule. Most of our experiments are
	conducted using the quick learning schedule with an exception
	for 100% MS-COCO protocol.

	We find that the model's performance is benefited significantly by
	longer training when more labeled training data and more
	complex data augmentation strategies are used.

	While there still exists the improvement in mAPs when STAC is trained
	with small amount of unlabeled data, the gain is less significant
	compared to that of supervised model with strong data augmentation.

	STAC trained with less accurate pseudo labels achieves only 24.25
	mAP, while the one with more accurate pseudo labels achieves 30.30 mAP,
	confirming the importance of pseudo label quality.

	While STAC brings a significant gain in mAP using pseudo labels,
	our results suggest that the incremental improvement on the quality of
	pseudo labels may not bring in a significant extra benefit.

	While STAC demonstrates an impressive performance gain already without
	taking confirmation bias [67, 1] issue into account, it could be
	problematic when using a detection framework with a stronger form
	of hard negative mining [48, 30] because noisy pseudo labels can
	be overly-used. Further investigation in learning with noisy labels,
	confidence calibration, and uncertainty estimation in the context of
	object detection are few important topics to further enhance the
	performance of SSL object detection.
		能提高常见/能学习识别出的物体的丰富性
			但对难例可能帮助不大
		需要大量得无标注数据
		延长训练对于增加数据和增强增广策略的最终效果有好处
-----------------------------------------------
OBB IoU
	Object-Detection-Metrics already modified to compute iou with obb
		@ 20220104
			to see more test log and thoughts
				F:\ws\notes-pytorch2021.txt


(efficientdet-pytorch) K:\ws\Object-Detection-Metrics>
python pascalvoc.py -gt DroneVehicle_tiny -det DroneVehicle_tiny_det -detformat obb_json -gtformat obb_json
python pascalvoc.py -gt 20200901M2_20200907_1202_200m_fixed_dataset -det 20200901M2_20200907_1202_200m_fixed_dataset_det -gtformat xyrb -detformat xyrb --savepath _results -t 0.75

re-evaluate oob mAP for all models

python pascalvoc.py -gt F:\ws\efficientdet_pytorch_win64\_datasets\_test_sets\private170 -det F:\ws\efficientdet_pytorch_win64\_datasets\_test_sets\private170_det_yolov5_obb -detformat obb_json -gtformat obb_json
(pytorch2022) F:\ws\Object-Detection-Metrics>python pascalvoc.py -gt F:\ws\efficientdet_pytorch_win64\_datasets\_test_sets\private170 -det F:\ws\efficientdet_pytorch_win64\_datasets\_test_sets\private170_det_yolov5_obb -detformat obb_json -gtformat obb_json


forget to exclude disabled markers in private170
	so mAP @ 20220104 should be all got updated.


yolov5_obb
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:| 99.55% | 99.52% | 99.45% | 99.35% | 99.00% | 98.11% | 95.05% | 80.24% | 34.19% |  0.80% | 80.52% | obb
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
SUMMARY:80.52
Spent 52.95 seconds on evaluating mAP on the test set.


model-z0011 vs model-073
    75.26%     81.89%
	the new model detects trucks better,
		but produces loose OBBs for all detections,
		which hurts high IoU mAPs a lot
			make OBB thinner by width*=0.908
				then 75.26% => 83.44%
					all recent(around 202209) trained have this issue
						model-z0009-054epoch, w*=0.923 => 84.51% (new BEST mAPs on OBBs) !!!
							all recent models use some of new datasets,
								which have more or less looser OBB/AABB than my internal dataset@2021
									the gap of vehicle width also amplified by heavy rot aug?
|model-073-036epoch, w*=0.923   | 99.86% | 99.79% | 99.75% | 99.58% | 99.11% | 98.80% | 97.59% | 89.50% | 51.62% |  1.60% | 83.72% | * +1.83
|model-092-035epoch, w*=0.923   | 99.69% | 99.65% | 99.65% | 99.37% | 99.09% | 98.81% | 97.41% | 89.14% | 46.43% |  1.27% | 83.05% | * +3.68
|model-z0009-054epoch, w*=0.923 | 99.79% | 99.67% | 99.48% | 99.33% | 99.11% | 98.60% | 97.62% | 92.05% | 56.74% |  2.72% | 84.51% | ** +6.2
|model-z0011-062epoch, w*=0.908 | 99.76% | 99.72% | 99.47% | 99.40% | 99.16% | 98.96% | 97.19% | 89.83% | 49.34% |  1.52% | 83.44% | * +8.18


092_reproduce_073_result_036epoch__bs8_2w_samples_per_virtual_epoch_train0.95_seed167
# try to reproduce 2021 best result with ML10 & 3090
# train82 with one more dataset: 0023_VSAI_dataset_2
# lr=0.0002, min_lr=lr/16, patient=1, max_epoch=40
# batch_size = 8, 40 virtual epochs with every 2w samples
cp train92.py train2.py
python train2.py 092_reproduce_073_result_036epoch__bs8_2w_samples_per_virtual_epoch_train0.95_seed167 167
[RESULT]: Train. Epoch: 35, summary_loss: 0.08144, time: 16.2 mins                          
[RESULT]: Val. Epoch: 35, summary_loss: 0.08149, time: 0.7 mins                       
2022-09-21T20:02:48.071539
LR: 5e-05
/home/me/1TSSD/maliang/efficientdet_pytorch/_models/092_reproduce_073_result_036epoch__bs8_2w_samples_per_virtual_epoch_train0.95_seed167/best-checkpoint-035epoch.bin
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:| 99.48% | 99.48% | 99.45% | 99.45% | 99.41% | 99.10% | 98.63% | 96.71% | 82.04% | 18.62% | 89.24% | aabb
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
SUMMARY:89.24


z0009_with_5_new_datasets_bs8_lr2e-4_2w_samples_per_virtual_epoch_train0.95_seed167/best-checkpoint-054epoch.bin
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:| 99.45% | 99.45% | 99.45% | 99.45% | 99.45% | 99.30% | 98.91% | 96.78% | 83.26% | 19.66% | 89.52% | aabb
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
# train82 with 5 new datasets including VAID_aabb
# lr=0.0002, min_lr=lr/16, patient=1, max_epoch=60
# batch_size = 8, 60 virtual epochs with every 2w samples
cp trainz0009.py train2.py
python train2.py z0009_with_5_new_datasets_bs8_lr2e-4_2w_samples_per_virtual_epoch_train0.95_seed167 167
   5000/   5000 0009_dataset_20200901M2_20200907_1202_200m_fixed_768_768_obb
   5000/   5000 0010_dataset_20200901M2_20200907_1202_200m_fixed_1536_768_obb
    250/    250 0011_dataset_20200901M2_20200907_1202_200m_fixed_768_768_obb_bus
    250/    250 0012_dataset_20200901M2_20200907_1202_200m_fixed_1536_768_obb_bus
   3124/   3124 0013_dataset_tongji_011_768_768_obb
   5000/   5000 0014_dataset_20200901M2_20200903_1205_250m_fixed_768_768_obb
   5000/   5000 0015_dataset_20200901M2_20200907_1104_200m_fixed_768_768_obb
   4940/   4940 0016_dataset_ysq1_768_768_obb
   4940/   4940 0017_dataset_ysq1_1440_768_obb
   8000/   8000 0018_syq4_dataset_768_768_obb_bus
   8000/   8000 0019_gm7_dataset_768_768_obb_bus
  11052/  11052 0020_web-collection-003_1184_768_768_obb
   1604/   1604 0022_UAV-ROD_dataset
    688/    688 0023_VSAI_dataset_2
   9708/   9708 0024_DroneVehicle_dataset
  10420/  10420 0025_VAID_dataset_aabb
   1923/   1923 0026_VEDAI_dataset
Training set:     80654
Test set:          4245
Batch Size:           8
Learning Rate: 0.000200
Num of Epoch:  60

# train82 with 4 new datasets except VAID_aabb
# lr=0.0002, min_lr=lr/32, patient=1, max_epoch=80
# batch_size = 8, 80 virtual epochs with every 2w samples, 90% rot_aug
# multistep_lr_schedule = [30,40,50,60,70]
cp trainz0011.py train2.py
python train2.py z0011_with_5_new_datasets_except_VAID_aabb_rotaug0.9_lr30-40-50-60-70_bs8_lr2e-4_2w_samples_per_virtual_epoch_train0.95_seed167 167
/home/me/1TSSD/maliang/efficientdet_pytorch/_models/z0011_with_5_new_datasets_except_VAID_aabb_rotaug0.9_lr30-40-50-60-70_bs8_lr2e-4_2w_samples_per_virtual_epoch_train0.95_seed167/best-checkpoint-062epoch.bin
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:| 99.48% | 99.48% | 99.48% | 99.44% | 99.41% | 99.22% | 98.69% | 96.39% | 79.72% | 18.15% | 88.95% | aabb
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
SUMMARY:88.95

1-4/65 = 0.938
1-5/65 = 0.932
1-6/65 = 0.908
1-7/65 = 0.892

OBB mAPs:
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
| model + w_scale               |   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-005-000epoch             | 91.76% | 89.41% | 87.60% | 85.85% | 83.37% | 79.79% | 71.06% | 42.25% |  7.87% |  0.02% | 63.90% | -
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-013-001epoch             | 98.14% | 97.85% | 97.34% | 96.48% | 95.46% | 94.13% | 90.93% | 79.91% | 38.18% |  1.90% | 79.03% | -
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-073-036epoch             | 99.83% | 99.75% | 99.67% | 99.30% | 98.96% | 98.11% | 95.86% | 83.43% | 41.60% |  2.34% | 81.89% | -
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-073-036epoch, w*=0.938   | 99.86% | 99.79% | 99.75% | 99.54% | 99.07% | 98.76% | 97.60% | 90.65% | 56.74% |  2.83% | 84.46% | * +2.56, datasets@2021
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-073-036epoch, w*=0.923   | 99.86% | 99.79% | 99.75% | 99.58% | 99.11% | 98.80% | 97.59% | 89.50% | 51.62% |  1.60% | 83.72% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-073-036epoch, w*=0.908   | 99.86% | 99.83% | 99.75% | 99.71% | 99.18% | 98.96% | 97.19% | 86.92% | 42.24% |  0.71% | 82.43% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-073-036epoch, w*=0.892   | 99.86% | 99.83% | 99.75% | 99.75% | 99.26% | 98.92% | 96.75% | 81.68% | 29.61% |  0.29% | 80.57% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-092-035epoch             | 99.69% | 99.65% | 99.46% | 99.25% | 98.86% | 97.88% | 92.81% | 72.45% | 31.53% |  2.13% | 79.37% | -
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-092-035epoch, w*=0.938   | 99.69% | 99.65% | 99.65% | 99.37% | 99.02% | 98.65% | 97.29% | 88.84% | 49.37% |  1.77% | 83.33% | * +3.96, one more dataset: 0023_VSAI_dataset_2
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-092-035epoch, w*=0.923   | 99.69% | 99.65% | 99.65% | 99.37% | 99.09% | 98.81% | 97.41% | 89.14% | 46.43% |  1.27% | 83.05% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-092-035epoch, w*=0.908   | 99.69% | 99.65% | 99.65% | 99.49% | 99.22% | 98.88% | 97.23% | 86.20% | 40.40% |  0.88% | 82.13% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-092-035epoch, w*=0.892   | 99.69% | 99.65% | 99.65% | 99.54% | 99.26% | 98.88% | 96.89% | 82.34% | 30.00% |  0.49% | 80.64% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0009-054epoch           | 99.70% | 99.56% | 99.33% | 99.16% | 98.59% | 97.83% | 93.74% | 70.47% | 23.84% |  0.88% | 78.31% | -
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0009-054epoch, w*=0.938 | 99.76% | 99.60% | 99.48% | 99.29% | 98.96% | 98.48% | 97.50% | 91.31% | 54.45% |  2.98% | 84.18% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0009-054epoch, w*=0.923 | 99.79% | 99.67% | 99.48% | 99.33% | 99.11% | 98.60% | 97.62% | 92.05% | 56.74% |  2.72% | 84.51% | ** +6.2, 5 new datasets including VAID_aabb
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0009-054epoch, w*=0.908 | 99.79% | 99.70% | 99.56% | 99.37% | 99.16% | 98.63% | 97.50% | 91.29% | 53.82% |  1.67% | 84.05% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0009-054epoch, w*=0.892 | 99.79% | 99.76% | 99.56% | 99.43% | 99.13% | 98.71% | 97.23% | 89.14% | 45.22% |  1.00% | 82.90% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0011-062epoch           | 99.76% | 99.47% | 99.35% | 99.20% | 98.91% | 97.39% | 88.64% | 55.36% | 14.00% |  0.47% | 75.26% | -
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0011-062epoch, w*=0.938 | 99.76% | 99.69% | 99.47% | 99.30% | 99.16% | 98.75% | 96.91% | 85.66% | 43.48% |  1.59% | 82.38% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0011-062epoch, w*=0.923 | 99.76% | 99.69% | 99.47% | 99.35% | 99.16% | 98.91% | 97.11% | 88.54% | 48.85% |  1.62% | 83.25% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0011-062epoch, w*=0.908 | 99.76% | 99.72% | 99.47% | 99.40% | 99.16% | 98.96% | 97.19% | 89.83% | 49.34% |  1.52% | 83.44% | * +8.18, 4 new datasets except VAID_aabb, 0.9 rotaug
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|model-z0011-062epoch, w*=0.892 | 99.76% | 99.76% | 99.47% | 99.40% | 99.16% | 99.00% | 97.06% | 89.12% | 46.93% |  1.30% | 83.10% |
+-------------------------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+

private 170 with vehicle types
model-z0009-054epoch
                 AP    TP    FP   DET
       car: 100.00%  2555    15  2555
       bus: 100.00%    69     0    69
 big_truck:  97.55%   169     6   173
     truck:  98.48%   130     0   132
========================================    50%
       car:  99.90%  2553    17  2555
       bus: 100.00%    69     0    69
 big_truck:  96.80%   168     7   173
     truck:  98.48%   130     0   132
========================================    55%
       car:  99.80%  2551    19  2555
       bus: 100.00%    69     0    69
 big_truck:  95.59%   166     9   173
     truck:  97.73%   129     1   132
========================================    60%
       car:  99.80%  2551    19  2555
       bus: 100.00%    69     0    69
 big_truck:  92.55%   163    12   173
     truck:  97.73%   129     1   132
========================================    65%
       car:  99.65%  2548    22  2555
       bus: 100.00%    69     0    69
 big_truck:  90.74%   161    14   173
     truck:  97.73%   129     1   132
========================================    70%
       car:  99.57%  2546    24  2555
       bus: 100.00%    69     0    69
 big_truck:  83.90%   154    21   173
     truck:  95.55%   127     3   132
========================================    75%
       car:  99.01%  2533    37  2555
       bus:  92.19%    66     3    69
 big_truck:  79.21%   150    25   173
     truck:  95.55%   127     3   132
========================================    80%
       car:  94.02%  2447   123  2555
       bus:  82.70%    62     7    69
 big_truck:  65.95%   136    39   173
     truck:  88.30%   121     9   132
========================================    85%
       car:  59.58%  1925   645  2555
       bus:  31.80%    37    32    69
 big_truck:  31.83%    94    81   173
     truck:  51.92%    86    44   132
========================================    90%
       car:   3.03%   394  2176  2555
       bus:   0.61%     5    64    69
 big_truck:   1.23%    15   160   173
     truck:   0.55%     8   122   132
========================================    95%
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:|   50%  |   55%  |   60%  |   65%  |   70%  |   75%  |   80%  |   85%  |   90%  |   95%  | 50-95% |
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
DETAIL:| 99.01% | 98.80% | 98.28% | 97.52% | 97.03% | 94.76% | 91.49% | 82.74% | 43.78% |  1.36% | 80.48% |
DETAIL:+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
SUMMARY:80.48

高IoU下的AP降低主要因为角度精度不够造成，也和车辆长度相关

\\192.168.2.22\WD_4T_red_2021\02_remapped_videos\_dataset_updates\private170_with_vehicle_types
\\192.168.2.22\WD_4T_red_2021\02_remapped_videos\_dataset_updates\private170_old (no vehicle types)
\\192.168.2.22\WD_4T_red_2021\02_remapped_videos\_dataset_updates\private170_2(by zhaoqiang & wangxinyu)


(pytorch2022) F:\ws\CyTrafficEditor2\Source\CyTrafficEditor\CyTraffic\data\videos>
	python compute_mIoU.py private170 private170_zqwxy
		matched[private170]:        [2936], mIoU:[92.4172%] 大量在[84.87%,98.85%]范围，尾部在[73.40%,84.87%]范围 峰值94.58%
			有少量不一致标注[mIoU==0%](估计跟我后期ps修改图片有关。。。)
	python compute_mIoU.py private170_zqwxy private170
		matched[private170_zqwxy]:  [2929], mIoU:[92.6686%] 大量在[84.78%,98.93%]范围，尾部在[73.40%,84.78%]范围 峰值94.58%


Q:\_papers_2022\data-centric\2021 - (VIP, very useful)Bootstrap Your Object Detector via Mixed Training.pdf
\\192.168.2.22\WD_4T_red_2021\_51vr_Liang_DL (57.7G)\_new\_20210421_debugging&metric
voxel51-fiftyone
fastdup

2019 - Prime Sample Attention in Object Detection.pdf
	focus on prime samples than hard samples when training a detector
		weight more on samples that have larger influnces on mAPs...
			In my opinion, to compare models with high mAPs, it's not suitable to use mAPs...
				precision vs recall
				localization/angle regression accurcy vs recall of rare objects

2021 - Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.pdf
2012 - Diagnosing Error in Object Detectors

浅析经典目标检测评价指标--mmAP（一）（二）
	https://zhuanlan.zhihu.com/p/55575423
	https://zhuanlan.zhihu.com/p/56899189
		mmAP是对每一个IOU阈值计算出一个mAP，然后将其平均得到最终的结果的，因此mAP@0.95和mAP@0.5是同等重要的
			（本文就暂以0.95和0.5举例，代表两个极端的阈值），虽然b中的两个检测结果其中必有一个为FP，但是在计算mAP@0.5时，
			分数低但定位更好的结果并不会对mAP产生影响（因为没有分数更低的TP出现了，该FP对P-R曲线毫无影响）；同时，
			在计算mAP@0.95时，分数高但定位更差的结果变成了FP，但是由于a中在该阈值下根本不存在TP，所以b的mAP仍高于a。
			在所有阈值下，b的mAP都要好于或等于a，所以会有b的mmAP高于a这样的反直觉的现象产生。
		保留那个分数低但是定位更好的，但是如何确定哪个是定位更好的结果呢？
			最简单的方法就是“预测定位效果”，IOU-Net就是一个可以用于解决这个问题的研究工作，
			通过预测检测结果与GT的IOU来判断哪个检测结果的定位效果更好；
		Soft-NMS
			重新打分的那些结果的平均性能优于该分数段的检测结果，那么性能就会提升
		mmAP也并不是全能的，有一个被mmAP忽略掉的因素就是“分数密度”，也就是每个目标的具体得分情况
			mmAP在计算时，只考虑所有检测结果的排序，但并不会考虑检测结果的具体分数，两个mmAP完全相同的检测器的得分可能相差很大
		mmAP是用来评价检测算法的，而acc是用来评价具体场景下的检测器的。
			检测算法的mmAP更高，那么它在综合所有任务场景上来看就会有更好的性能。但是当我们有一个确定的场景时，mmAP就会因为
			“考虑得太全面”而不那么适用了，此时我们应该寻找一个其他的评价指标来衡量检测器的性能，这个指标需要考虑很多因素，
			比如如何在P-R重要性之间进行trade-off（取舍）。

cleanlab

实战工具|目标检测评价mAP精细化系列工具
	https://mp.weixin.qq.com/s?__biz=Mzg5NzgyNTU2Mg==&mid=2247498363&idx=2&sn=5149fab2b2c67327dda2d222435d95dd&source=41#wechat_redirect
		FiftyOne
		COCO Analysis Toolkit
		UAP
			2019 - Empirical Upper Bound in Object Detection and More
				empirical upper bound in AP is
					the score of a detector with ground truth bounding boxes labeled by the best object classifier.
						The classification score is considered as the detection score. This way we essentially
						assume that the localization problem is solved and what remains is only object recognition.
				the performance gap is larger over small objects,
					indicating that scale is one of the major problems in object detection
				the bottleneck in object detection is object recognition
				detection models lack generalization in terms of searching the right places,
					utilizing context, recognition of small objects, and robustness to image transformation.
				We did not find a significant contribution from the surrounding
					context of a target or its nearby overlapping boxes to better classify it.
		TIDE
			2020 - TIDE - A General Toolbox for Identifying Object Detection Errors

2021 - Scale-aware Automatic Augmentation for Object Detection
	box-level augmentation
		image
		geometry

2022 - Sensitivity of Average Precision to Bounding Box Perturbations
2022 - Pareto Refocusing for Drone-view Object Detection
	improve the TIDE [27], which can figure out the percentage of different errors in the object detection, such as false positives, etc.
2018 - Localization Recall Precision (LRP) - A New Performance Metric for Object Detection
	AP cannot distinguish between very different RP curves.
	Another deficiency of AP
		it does not explicitly include localization accuracy.
	AP is not confidence-score sensitive.
	AP does not suggest a confidence score threshold for the best setting of the object detector.
	AP uses interpolation between neighboring recall values.
		which is especially problematic for classes with very small size.
	higher τ means we require tighter BBs. Overall, both parameters are related with the RP curve: 
		A τ value corresponds to drawing the RP curve and
		an s value determines a point on the RP curve to evaluate in terms of the LRP error.
2021 - One Metric to Measure them All - Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks
2020 - (master) Training object detectors by directly optimizing LRP metric
2022 - (master) Correlation Loss - Enforcing Correlation Between Classification and Localization in Object Detection
2020 - Labels Are Not Perfect - Inferring Spatial Uncertainty in Object Detection
2020 - Inferring Spatial Uncertainty in Object Detection
2021 - (VIP)(Dissertation_Di_Feng) Uncertainty estimation for object detection using deep learning approaches
2022 - (VIP) Common Limitations of Image Processing Metrics - A Picture Story
2020 - Probabilistic Object Detection - Definition and Evaluation
2020 - (VIP)Imbalance Problems in Object Detection - A Review

lr_finder
https://github.com/rwightman/efficientdet-pytorch/issues/119
	How to run LR finder? #119
	Beyond the scope of this repository. The bench contains criterion and outputs the losses in output as dict keys,
	you'd likely have to modify the finder to access the loss that way.

使用lr_finder得到的初始最大学习率在
	bs4, datasets@2021
		0.0004到0.0005之间
	bs4, dataset@2021 + 5_new_datasets
		0.0003到0.0004之间
	bs4, dataset@2021 + 5_new_datasets, rotaug0.9
		0.0003到0.0004之间
	bs1, accumulation_steps=4, dataset@2021 + 5_new_datasets
		0.0003到0.0004之间
	bs8, dataset@2021 + 5_new_datasets
		0.0003到0.0004之间	从1e-6找到1e-2，1k iterations
		0.0002到0.0003之间	从1e-4找到1e-2，1k iterations
	bs8, dataset@2021 + 5_new_datasets, accumulation_steps=4
	bs8, dataset@2021 + 5_new_datasets, accumulation_steps=8
	bs8, dataset@2021 + 5_new_datasets, accumulation_steps=6

	model training resume
	lr_finder in the mid of training
		very noisy with small lr???
			bigger accumulation_steps?


(pytorch2022) F:\ws\CyTrafficEditor2\Source\CyTrafficEditor\CyTraffic\data\videos>
	python compute_mIoU.py _relabeled\073\linz_det _relabeled\092\linz_det
		073\linz_det vs 092\linz_det:	matched[_relabeled\073\linz_det]:[1829], mIoU:[78.8845%]
		092\linz_det vs 073\linz_det:	matched[_relabeled\092\linz_det]:[1723], mIoU:[83.8404%]
			073去匹配092时匹配数目少但mIoU高，说明073漏检多，（但073框比较准确么? 需要用有gt标注的进一步验证）

	073\Munich3K_det vs 092\Munich3K_det:	matched[_relabeled\073\Munich3K_det]:[4040], mIoU:[83.7098%]
	092\Munich3K_det vs 073\Munich3K_det:	matched[_relabeled\092\Munich3K_det]:[3927], mIoU:[86.2603%]

private170_gt private170_det(model-073-036epoch):	matched[private170_det(a_gt_vis)]:[2956], mIoU:[89.5207%]
private170_det(model-073-036epoch) private170_gt:	matched[private170_det(model-073-036epoch)]:[2929], mIoU:[90.1817%]

private170_gt private170_det(model-092-035epoch):	matched[private170_gt]:[2954], mIoU:[88.7856%]
private170_det(model-092-035epoch) private170_gt：	matched[private170_det(model-092-035epoch)]:[2929], mIoU:[89.4466%]

private170_det(model-092-035epoch) private170_det(model-073-036epoch):	matched[private170_det(model-092-035epoch)]:[2956], mIoU:[94.2790%]
private170_det(model-073-036epoch）private170_det(model-092-035epoch)：	matched[private170_det(model-073-036epoch)]:[2954], mIoU:[94.2696%]
	在private170上073与092漏检差异不大，主要差异在mIoU上
		073在mIoU上比092高0.5%


进一步分析长宽分布和差异，统计漏检与误检
	参看fiftyone的效果


https://github.com/ildoonet/pytorch-gradual-warmup-lr

re-check the lr_finder paper
2017 - Cyclical Learning Rates for Training Neural Networks
https://arxiv.org/pdf/1506.01186.pdf
3.3. How can one estimate reasonable minimum and maximum boundary values?
	There is a simple way to estimate reasonable minimum
	and maximum boundary values with one training run of the
	network for a few epochs. It is a “LR range test”; run your
	model for several epochs while letting the learning rate increase linearly between low and high LR values. This test
	is enormously valuable whenever you are facing a new architecture or dataset.
	Figure 3. Classification accuracy as a function of increasing learning rate for 8 epochs (LR range test).

	Whenever one is starting with a new architecture or
	dataset, a single LR range test provides both a good LR
	value and a good range. Then one should compare runs with
	a fixed LR versus CLR with this range. Whichever wins can
	be used with confidence for the rest of one’s experiments.
		for several epochs?
			ok, try to search other docs on web...

didn't reasonable lr with lr_finder
	different max_iterations got very different lr range
need a warmup? or I use lr_finder in a wrong way...
CosineAnnealingLR

lr_find 原始论文是使用SGD+momentum，adam有自适应学习率的逻辑，能和lr_finder一起使用么？台阶step降低是不是有影响？
https://forums.fast.ai/t/is-learning-rate-finder-works-for-adam/7976
	Jeremy Howard
		Huh - guess I misremembered. I think at some point I had Adam as default, but looks like I switch to SGD with momentum. How embarassing!
			Well spotted. However I find these theoretical papers using synthetic datasets nearly always useless in practice. A paper from a few days ago shows we can use Adam to get SoTA results: https://arxiv.org/abs/1711.05101 48 . I’d like to ideally implement that before I make Adam the default, since without it, Adam can give slightly sub-optimal results.
				2017 - Decoupled Weight Decay Regularization
					the AdamW paper
						https://github.com/loshchil/AdamW-and-SGDW
				https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html
					@ 2018-07-02
					1cycle policy + AdamW
						so lr_finder is used?



最大lr选择
	lr_finder图中，loss下降最快处的lr，还是loss上升前的最大lr?





https://www.reddit.com/r/MachineLearning/comments/pp0itx/d_interpreting_lr_finder_plot_for_one_cycle_lr/
	[D] Interpreting lr finder plot for one cycle LR scheduler
	I am experimenting with One Cycle lr scheduler and I am observing certain trends that I don't see discussed
	anywhere. I want to share what I have seen in case anyone else has seen something similar or if it is just
	something specific to the architecture and the dataset I am working on.

	Regarding the use of lr finder for choosing learning rates. As a rule of thumb for using lr finder, it is
	suggested to use the point of higher gradient if using a constant value optimization algorithm, when using
	one cycle it is suggested to use a value near the minimum before divergence as the max lr. But no importance
	is given to the number of steps.

	As you can see in the image the max gradient and divergence points change a lot for different n values.
	Furthermore, I have been able to increase the max_lr as high as 1e3 without issues, and if divergence occurs
	it can be evaded just by increasing the number of epochs leading to better results, suggesting that what the
	limitation really is here is the change in the learning rate.

	So how should we really interpret the lr finder plot? Is there a rule of thumb for extrapolating the behavior
	for a certain number of steps to a certain number of epochs?	
	有人遇到相似的问题 @ about 202110
		不同iteration得到的lr_finder图差别很大，难以决定具体使用哪个作为最大lr

https://forums.fast.ai/t/selecting-learn-rates-in-fastai/51929
https://wandb.ai/ayush-thakur/debug-neural-nets/reports/Visualizing-and-Debugging-Neural-Networks-with-PyTorch-and-W-B--Vmlldzo2OTUzNA
https://towardsdatascience.com/speeding-up-neural-net-training-with-lr-finder-c3b401a116d0
https://adityassrana.github.io/blog/pytorch/2021/03/31/OneCycle-Training.html
https://github.com/whai362/PVT/issues/18
	different lr with SGD/Adam/AdamW
	todo read





关于lr_finder的原理讨论
https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html
	@ 20 March 2018
	So we have to pick exactly the right value, not too high and not too low. For a long time, it's been a game
	of try and see, but in this article (2017 - Cyclical Learning Rates for Training Neural Networks) another
	approach is presented. Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it
	(by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value
	(like 1 or 10). Record the loss each time at each iteration and once you're finished, plot those losses
	against the learning rate.

	The loss decreases at the beginning, then it stops and it goes back increasing, usually extremely quickly.
	That's because with very low learning rates, we get better and better, especially since we increase them.
	Then comes a point where we reach a value that's too high and the phenomenon shown before happens. Looking
	at this graph, what is the best learning rate to choose? Not the one corresponding to the minimum.

	Why? Well the learning rate that corresponds to the minimum value is already a bit too high, since we are
	at the edge between improving and getting all over the place. We want to go one order of magnitude before,
	a value that's still aggressive (so that we train quickly) but still on the safe side from an explosion.
	In the example described by the picture above, for instance, we don't want to pick 10−1 but rather 10−2.

	This method can be applied on top of every variant of SGD, and any kind of network. We just have to go through
	one epoch (usually less) and record the values of our loss to get the data for our plot.
		不使用loss最低处的lr，而是再降低一个数量级，确保lr安全的同时尽量大
https://forums.fast.ai/t/learning-rate-finders-mathematical-theory/56738
	My question is,
		There is no description of Learning Rate Finder 41 in this article (Cyclical Learning Rates for Training Neural Networks 12), even there is no image ‘learning rate’ vs ‘loss’ in it. Why?
		Where does the article describe the problem with the Learning Rate Finder 41?

		In the process of Learning Rate Finder 41,
		one step (learning rate changes) is synchronized with one mini-batch. Is it right?
		If yes: the input image to model is not Invariant, how can we know the change of ‘loss’ is because of learning rate but not the change of input image?
		If no: one step (learning rate changes) is synchronized with what?

		In the process of Learning Rate Finder 41,
		Will the weight of the model be updated?
		If yes: the ‘loss’ will become smaller and smaller, even ‘learning rate’ doesn’t change.

		The value of ‘loss’ in image ‘learning rate’ vs ‘loss’, is the average of the entire output node, is that right?
	I am pretty sure that lr finder has no real mathematical explanation, it is very empirical. They noticed that the lr it
	choses works very well with 1-cycle policy, at least when tested on kind of simple tasks (and with frozen layers). As
	for your questions:
		lr steps are indeed synchronized with mini-batches, which doesn’t ensure that variations in loss are due to varations in lr. However:
			with big enough batches, it should not be much of a problem as long as data is not too imbalanced
			You can use as many steps as you want, so that the variation on each batch gets minimal and you mitigate the effect of outliers.
			The goal is to get an overall profile, so we look at a smoothened version of the loss which mitigates the effect of outlier batches.
		The weights get updated (well except those that are frozen), but we want to measure how fast they get changed, which mostly depends on lr. Still, there are other factors, such as the loss profile and indeed the current state of the model.
		No, the value is a smoothened value of loss, taken from an exponential moving average of the loss (with a momentum of 0.98). You can check callback.SmoothenValue to get the formula.
	Overall, LRFinder has multiple holes if we try to explain it theoretically but it still gives good empirical results. It doesn’t work all the time though, and is obviously not perfect. Someone from fastai could answer this far better than me however, I am just guessing from the videos I watched and my personal usage of it.



try SGD+momentum with OnCycleLR
training set 57528, 4 batch size, 14382 iterations
lr0.6=>0.6/100000 14epochs:	got nan loss
lr0.4=>0.4/100000 14epochs:	got nan loss
lr0.06=>0.06/100000 14epochs:	

lr0.056=>0.0056/25 1epoch:	85.92%
lr0.056=>0.0056/25 4epochs:	87.12%
lr0.056=>0.0056/25 8epoch:	87.54%
lr0.056=>0.0056/25 12epoch:	

https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html


使用不同的总训练长度 1epoch vs 14 epochs，前10%的sample用于lr上升的情况下
	训练中loss不变nan的最大lr也不一样
		跟div_factor也有关
			总之预热的不同程度后能使用的最大lr不一样
	1 epoch:   0.0056 (div_factor=25)
	14 epochs: 0.6 (div_factor=100000)
		todo：改好tensorboard，看lr/loss/mAP走势图




efficientdet-pytorch/train.py
	try this?

efficientdet-pytorch some weight frozen? weight decay changed? wheat detection original code?



clean up old dataset @ 2021 ?
clean up / relabel some of 5 new datasets ?
unsupervised clustering ?
fastdup
cleanlab
detect heading

regression IoU of detection
calibrate uncertainty

new vehicle detection metric ?
	left bound
	right bound
	head bound
	back bound
	heading angle

shrink OBB width of new dataset in training?

exponential moving average (EMA)
	2020 - Self-training with noisy student improves imagenet classification

2021 - A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit
	mean Average Precision (mAP)
	Average Recall (AR)
	Spatio-Temporal Tube Average Precision (STT-AP)
2022 - Optimal Correction Cost for Object Detection Evaluation
	mAP does not treat each image equally
	mAP does not penalizes incorrect detections ranked lower than correct ones
2020 - Reducing Label Noise in Anchor-Free Object
	We sum-pool predictions stemming from individual features into a single prediction.


mAP精度对角度精度不敏感，仅在85%以上IoU阈值条件下，随角度误差增大而缓慢降低
	需要利用帧间连续一致性，来保持角度稳定
		角度精度对大车mAP有较大影响，但目前大车本身检出率也比较低。。。

efficientdet + OBB ?
检测AABB同时回归角度 ?


检测误差分类
	大车漏检
	密集车辆误检和漏检
	车辆包围盒不够紧密
	车辆朝向不准（+/-5度）

数据采样：
记录修改过的预标注的数据，基于修改iou的crop采样
平衡batch中的数据
划分样本，先少后多，先易后难，batch中用类似样本
分层抽样
more negative sample

dataset exploration
	random scaling might improve mAP on scales that other than 60 pixel
	very inbalanced groups
		1000 : 1 or even worse
	balanced sample in one batch
	for most common cars
		3 major resolution in dataset
			27 * 11
			47 * 20
			68 * 27
		1 major aspect ratio
			2.4 : 1


数据分析：
length/width ratio of false positive
locatiion distribution
gps distribution
country distribution
Model Card Toolkit

增广：
训练时在线random crop
在线旋转切图以及扭曲图像
利用gan做车辆分布增广
比较人为标记间的差异

集成：
weighted box fusion ensemble
Stochastic Weights Averaging (SWA)
----------------------------------------------------------------------------------------------

20221005
efficientdet_pytorch_win64/infer4.py
	no-cropping version
	inference on the whole image in one batch
	but the results got much worse than cropping versions...
		more puposal but less final detections...
			it's due to
				F:\ws\efficientdet_pytorch_win64\effdet\anchors.py
				# The maximum number of detections per image.
				MAX_DETECTIONS_PER_IMAGE = 100
					For gm7, it has 300+ vehicles in the image...
						so just set 500 and the results are fine now!!!


(Pdb) bt
  c:\anaconda3\envs\pytorch2022\lib\runpy.py(197)_run_module_as_main()
-> return _run_code(code, main_globals, None,
  c:\anaconda3\envs\pytorch2022\lib\runpy.py(87)_run_code()
-> exec(code, run_globals)
  f:\ws\efficientdet_pytorch_win64\infer4.py(702)<module>()
-> main()
  f:\ws\efficientdet_pytorch_win64\infer4.py(699)main()
-> run(sys.argv[1], [], common_vehicle_width=(float(sys.argv[2]) if len(sys.argv) > 2 else None), model_path=(sys.argv[3] if len(sys.argv) > 3 else None))
  f:\ws\efficientdet_pytorch_win64\infer4.py(460)run()
-> results = predict(net, config, angle=0.0, img_path=img_path, want_aabb=True, delay=0)
  f:\ws\efficientdet_pytorch_win64\infer4.py(238)predict()
-> predictions = make_predictions(images, net)
  f:\ws\efficientdet_pytorch_win64\infer4.py(65)make_predictions()
-> det = net(images, img_scale, img_size)
  c:\anaconda3\envs\pytorch2022\lib\site-packages\torch\nn\modules\module.py(1102)_call_impl()
-> return forward_call(*input, **kwargs)
  f:\ws\efficientdet_pytorch_win64\effdet\bench.py(76)forward()
-> class_out, box_out = self.model(x)
  c:\anaconda3\envs\pytorch2022\lib\site-packages\torch\nn\modules\module.py(1102)_call_impl()
-> return forward_call(*input, **kwargs)
> f:\ws\efficientdet_pytorch_win64\effdet\efficientdet.py(478)forward()
-> x = self.backbone(x)
(Pdb) x.shape
torch.Size([1, 3, 2176, 3840])


3840x2176

(Pdb) x.shape
torch.Size([1, 3, 2176, 3840])
(Pdb) x.shape[0]
1
(Pdb) x[0].shape
torch.Size([3, 2176, 3840])
(Pdb) zz = x[0].permute(1,2,0).cpu().numpy()
(Pdb) import cv2
(Pdb) cv2.imshow("result", zz)
(Pdb) k = cv2.waitKey()


(Pdb) x[0].shape
torch.Size([1, 48, 272, 480])
(Pdb) x[1].shape
torch.Size([1, 120, 136, 240])
(Pdb) x[2].shape
torch.Size([1, 352, 68, 120])
(Pdb) bt
  c:\anaconda3\envs\pytorch2022\lib\runpy.py(197)_run_module_as_main()
-> return _run_code(code, main_globals, None,
  c:\anaconda3\envs\pytorch2022\lib\runpy.py(87)_run_code()
-> exec(code, run_globals)
  f:\ws\efficientdet_pytorch_win64\infer4.py(701)<module>()
-> main()
  f:\ws\efficientdet_pytorch_win64\infer4.py(698)main()
-> run(sys.argv[1], [], common_vehicle_width=(float(sys.argv[2]) if len(sys.argv) > 2 else None), model_path=(sys.argv[3] if len(sys.argv) > 3 else None))
  f:\ws\efficientdet_pytorch_win64\infer4.py(459)run()
-> results = predict(net, config, angle=0.0, img_path=img_path, want_aabb=True, delay=0)
  f:\ws\efficientdet_pytorch_win64\infer4.py(237)predict()
-> predictions = make_predictions(images, net)
  f:\ws\efficientdet_pytorch_win64\infer4.py(65)make_predictions()
-> det = net(images, img_scale, img_size)
  c:\anaconda3\envs\pytorch2022\lib\site-packages\torch\nn\modules\module.py(1102)_call_impl()
-> return forward_call(*input, **kwargs)
  f:\ws\efficientdet_pytorch_win64\effdet\bench.py(76)forward()
-> class_out, box_out = self.model(x)
  c:\anaconda3\envs\pytorch2022\lib\site-packages\torch\nn\modules\module.py(1102)_call_impl()
-> return forward_call(*input, **kwargs)
> f:\ws\efficientdet_pytorch_win64\effdet\efficientdet.py(479)forward()
-> x = self.fpn(x)
5
(Pdb) x[0].shape
torch.Size([1, 112, 272, 480])
(Pdb) x[1].shape
torch.Size([1, 112, 136, 240])
(Pdb) x[2].shape
torch.Size([1, 112, 68, 120])
(Pdb) x[3].shape
torch.Size([1, 112, 34, 60])
(Pdb) x[4].shape
torch.Size([1, 112, 17, 30])



768x768 images:
(Pdb) x.shape
torch.Size([32, 3, 768, 768])
(Pdb) n
> f:\ws\efficientdet_pytorch_win64\effdet\efficientdet.py(479)forward()
-> x = self.fpn(x)
(Pdb) len(x)
3
(Pdb) x[0].shape
torch.Size([32, 48, 96, 96])
(Pdb) x[1].shape
torch.Size([32, 120, 48, 48])
(Pdb) x[2].shape
torch.Size([32, 352, 24, 24])
(Pdb) n
> f:\ws\efficientdet_pytorch_win64\effdet\efficientdet.py(480)forward()
-> x_class = self.class_net(x)
(Pdb) len(x)
5
(Pdb) x[0].shape
torch.Size([32, 112, 96, 96])
(Pdb) x[1].shape
torch.Size([32, 112, 48, 48])
(Pdb) x[2].shape
torch.Size([32, 112, 24, 24])
(Pdb) x[3].shape
torch.Size([32, 112, 12, 12])
(Pdb) x[4].shape
torch.Size([32, 112, 6, 6])

F:\ws\efficientdet_pytorch_win64\effdet\anchors.py
def generate_detections(
        cls_outputs, box_outputs, anchor_boxes, indices, classes,
        img_scale: Optional[torch.Tensor], img_size: Optional[torch.Tensor],
        max_det_per_image: int = MAX_DETECTIONS_PER_IMAGE, soft_nms: bool = False):


draw oob
(pytorch2022) F:\ws\efficientdet_pytorch_win64>
python draw_obb.py _datasets\_test_sets\private170 _datasets\_test_sets\private170_det(model-005-000epoch)
python draw_obb.py _datasets\_test_sets\private170 _datasets\_test_sets\private170_det(model-013-001epoch)
python draw_obb.py _datasets\_test_sets\private170 _datasets\_test_sets\private170_det(model-073-036epoch)
python draw_obb.py _datasets\_test_sets\private170 _datasets\_test_sets\private170_det(model-092-035epoch)
python draw_obb.py _datasets\_test_sets\private170 _datasets\_test_sets\private170_det(model-z0009-054epoch)
python draw_obb.py _datasets\_test_sets\private170 _datasets\_test_sets\private170_det(model-z0011-062epoch)


draw FP


todo
	eval obb mAP on training